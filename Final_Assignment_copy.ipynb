{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helio Inaba - CQF June 2023 Final Project \n",
    "## Optimal Hedging with Advanced Delta Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from IPython.display import Latex\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "#pd.set_option('display.max_columns', None)\n",
    "pd.reset_option('^display.', silent=True)\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(data, bins):\n",
    "\n",
    "    #Returns a normal distribution with the same mean, variance and number of divisions of a given dataset.\n",
    "    \n",
    "    return 1/(data.std()*np.sqrt(2*np.pi))*(np.exp(-(bins - data.mean())**2/(2*data.std()**2)))\n",
    "\n",
    "\n",
    "def black_scholes(S, K, sigma, T, r, option_type='call'):\n",
    "\n",
    "    #Returns the Black-Scholes Price of an option.\n",
    "\n",
    "    d1 = ((np.log(S/K)+(r+(sigma**2)/2)*T)/(sigma*np.sqrt(T)))\n",
    "    d2 = d1-sigma*np.sqrt(T)\n",
    "    n_d1 = norm.cdf(d1)\n",
    "    n_d2 = norm.cdf(d2)\n",
    "\n",
    "    if option_type == 'call':\n",
    "        V = n_d1*S-n_d2*K*np.exp(-r*T)\n",
    "    elif option_type == 'put':\n",
    "        V = (1-n_d2)*K*np.exp(-r*T) - S*(1-n_d1)\n",
    "    else:\n",
    "        raise ValueError('Option Type not supported')\n",
    "    return V\n",
    "\n",
    "\n",
    "def bs_delta(S, K, sigma, T, r):\n",
    "\n",
    "    #Returns the Black-Scholes Delta of an option.\n",
    "\n",
    "    d1 = ((np.log(S/K)+(r+(sigma**2)/2)*T)/(sigma*np.sqrt(T)))\n",
    "    n_d1 = norm.cdf(d1)\n",
    "    \n",
    "    return n_d1\n",
    "\n",
    "\n",
    "def bs_gamma(S, K, sigma, T, r):\n",
    "\n",
    "    #Returns the Black-Scholes Gamma of an option\n",
    "\n",
    "    d1 = ((np.log(S/K)+(r+(sigma**2)/2)*T)/(sigma*np.sqrt(T)))\n",
    "    \n",
    "    return norm.pdf(d1)/(S*sigma*np.sqrt(T))\n",
    "\n",
    "\n",
    "def bs_vega(S, K, sigma, T, r):\n",
    "\n",
    "    #Returns the Black-Scholes Vega of an option.\n",
    "\n",
    "    d1 = ((np.log(S/K)+(r+(sigma**2)/2)*T)/(sigma*np.sqrt(T)))\n",
    "    \n",
    "    return S*norm.pdf(d1)*np.sqrt(T)\n",
    "\n",
    "\n",
    "def bs_theta(S, K, sigma, T, r):\n",
    "\n",
    "    #Returns the Black-Scholes Theta of an option.\n",
    "\n",
    "    d1 = ((np.log(S/K)+(r+(sigma**2)/2)*T)/(sigma*np.sqrt(T)))\n",
    "    d2 = d1-sigma*np.sqrt(T)\n",
    "    n_d2 = norm.cdf(d2)\n",
    "\n",
    "    theta = -S*norm.pdf(d1)*sigma/(2*np.sqrt(T))\n",
    "    theta -= r*K*np.exp(-r*T)*n_d2\n",
    "\n",
    "    return theta\n",
    "\n",
    "def bs_rho(S, K, sigma, T, r):\n",
    "\n",
    "    #Returns the Black-Scholes Rho of an option.\n",
    "\n",
    "    d1 = ((np.log(S/K)+(r+(sigma**2)/2)*T)/(sigma*np.sqrt(T)))\n",
    "    d2 = d1-sigma*np.sqrt(T)\n",
    "    n_d2 = norm.cdf(d2)\n",
    "\n",
    "    return K*T*np.exp(-r*T)*n_d2\n",
    "\n",
    "\n",
    "def euler_maruyama(s0, expiry_T, sigma, return_rate, n_periods, n_simulations):\n",
    "\n",
    "    #returns n_simulations prices paths with n_periods time intervals each using the Euler-Maruyama discretization method.\n",
    "\n",
    "    prices = np.zeros((n_periods, n_simulations))\n",
    "    prices[0] = s0\n",
    "\n",
    "    for i in range(1, n_periods):\n",
    "\n",
    "        prices[i] = prices[i-1]*(1 + return_rate*expiry_T/n_periods + sigma*np.sqrt(expiry_T/n_periods)*np.random.normal(loc=0.0, scale=1.0, size=n_simulations))\n",
    "\n",
    "    prices = pd.DataFrame(prices)\n",
    "    prices.index.name = 'period'\n",
    "    prices.index.columns = 'simulation'\n",
    "\n",
    "    return prices\n",
    "\n",
    "\n",
    "def milstein(s0, expiry_T, sigma, return_rate, n_periods, n_simulations):\n",
    "\n",
    "    #returns n_simulations prices paths with n_periods time intervals each using the Milstein discretization method.\n",
    "    \n",
    "    prices = np.zeros((n_simulations, n_periods+1))\n",
    "    prices[:,0] = s0\n",
    "\n",
    "    A_matrix = np.sqrt(expiry_T/n_periods)*np.tri(n_periods,n_periods)\n",
    "    z = np.random.normal(loc=0.0, scale=1.0, size=(n_periods,n_simulations))\n",
    "\n",
    "    wiener_matrix = np.matmul(A_matrix, z)\n",
    "    wiener_matrix = np.transpose(wiener_matrix)\n",
    "\n",
    "    prices[:,1:] = s0*np.exp((return_rate-(sigma**2)/2)*np.linspace(start=expiry_T/n_periods,stop=expiry_T,num=n_periods)+sigma*wiener_matrix)\n",
    "\n",
    "    prices = pd.DataFrame(np.transpose(prices))\n",
    "    prices.index.name = 'period'\n",
    "    prices.index.columns = 'simulation'\n",
    "\n",
    "    return prices\n",
    "\n",
    "def antithetic_variables(s0, expiry_T, sigma, return_rate, n_periods, n_simulations):\n",
    "\n",
    "    #returns n_simulations prices paths with n_periods time intervals each using the Milstein discretization method.\n",
    "    #The Milstein scheme is run for n_simulations/2 paths and its stochastic portion is replicated with opposite side for the other half of simulations.\n",
    "    \n",
    "    \n",
    "    prices_plus = np.zeros((n_simulations, n_periods+1))\n",
    "    prices_plus[:,0] = s0\n",
    "\n",
    "    prices_minus = np.zeros((n_simulations, n_periods+1))\n",
    "    prices_minus[:,0] = s0\n",
    "\n",
    "    A_matrix = np.sqrt(expiry_T/n_periods)*np.tri(n_periods,n_periods)\n",
    "    z = np.random.normal(loc=0.0, scale=1.0, size=(n_periods,n_simulations))\n",
    "    z_minus = -z\n",
    "\n",
    "    wiener_matrix = np.matmul(A_matrix, z)\n",
    "    wiener_matrix = np.transpose(wiener_matrix)\n",
    "\n",
    "    wiener_matrix_minus = np.matmul(A_matrix, z_minus)\n",
    "    wiener_matrix_minus = np.transpose(wiener_matrix_minus)\n",
    "\n",
    "    prices_plus[:,1:] = s0*np.exp((return_rate-(sigma**2)/2)*np.linspace(start=expiry_T/n_periods,stop=expiry_T,num=n_periods)+sigma*wiener_matrix)\n",
    "    prices_minus[:,1:] = s0*np.exp((return_rate-(sigma**2)/2)*np.linspace(start=expiry_T/n_periods,stop=expiry_T,num=n_periods)+sigma*wiener_matrix_minus)\n",
    "\n",
    "    prices_plus = pd.DataFrame(np.transpose(prices_plus))\n",
    "    prices_minus = pd.DataFrame(np.transpose(prices_minus))\n",
    "    \n",
    "    prices_plus.index.name = 'period'\n",
    "    prices_plus.index.columns = 'simulation'\n",
    "\n",
    "    prices_minus.index.name = 'period'\n",
    "    prices_minus.index.columns = 'simulation'\n",
    "\n",
    "    return prices_plus, prices_minus\n",
    "\n",
    "\n",
    "def fill_bridge_array(a, b, delta_t, z=None):\n",
    "\n",
    "    #Returns a Brownian Bridge connecting point a to point b.\n",
    "    #It can either receive or generate a random variable that fills the gap.\n",
    "    \n",
    "    if type(z) == type(None):\n",
    "        return (a+b)/2 + np.sqrt((delta_t/4))*np.random.normal(loc=0.0, scale=1.0, size=1)[0]\n",
    "    else:\n",
    "        return (a+b)/2 + np.sqrt((delta_t/4))*z\n",
    "\n",
    "def sobol_prices(s0, expiry_T, sigma, return_rate, n_periods, n_simulations):\n",
    "\n",
    "    #returns n_simulations prices paths with n_periods time intervals using Sobol variables.\n",
    "    #For each path simulation, it firsts uses the first number of its sequence to generate the final random point, then fills the path using the fill_bridge_array.\n",
    "\n",
    "\n",
    "    n = int(np.log2(n_periods))\n",
    "\n",
    "    compounding_array = np.exp((return_rate-(sigma**2)/2)*np.linspace(start=expiry_T/n_periods,stop=expiry_T,num=n_periods))\n",
    "\n",
    "    W_t = np.empty((n_periods+1, n_simulations))\n",
    "    W_t[:] = np.nan\n",
    "    W_t[0] = 0.0\n",
    "\n",
    "    prices = np.zeros((n_periods+1, n_simulations))\n",
    "    prices = pd.DataFrame(prices, index=np.arange(0, 1+1/(2**n), 1/(2**n)))\n",
    "    prices.iloc[0] = s0\n",
    "\n",
    "    sampler = scipy.stats.qmc.Sobol(d=n_periods, scramble=True)\n",
    "    sample = sampler.random_base2(int(np.log2(n_simulations)))\n",
    "    sample = np.transpose(sample)\n",
    "    norm_sample = norm.ppf(sample)\n",
    "\n",
    "    k = 0\n",
    "    W_t[-1,:] = norm_sample[k,:]\n",
    "\n",
    "    df_wt = pd.DataFrame(W_t, index=np.arange(0, 1+1/(2**n), 1/(2**n)))\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        \n",
    "        for j in ((1/2)**i)*np.arange(1, 2**(i)+1, 2):\n",
    "            k+=1\n",
    "            df_wt.loc[j] = fill_bridge_array(a=df_wt.loc[j-(1/2)**i], b=df_wt.loc[j+(1/2)**i], delta_t=(1/2)**(i-1), z=norm_sample[k,:])\n",
    "            \n",
    "    prices.iloc[1:] = s0*(np.exp(sigma*df_wt.iloc[1:,].values).T*compounding_array).T\n",
    "    prices.index = prices.index*expiry_T\n",
    "\n",
    "    prices.index.name = 'period'\n",
    "    prices.index.columns = 'simulation'\n",
    "\n",
    "    return prices\n",
    "\n",
    "\n",
    "def fill_bridge(a, b, delta_t, z=None):\n",
    "\n",
    "    \n",
    "    if z == None:\n",
    "        return (a+b)/2 + np.sqrt((delta_t/4))*np.random.normal(loc=0.0, scale=1.0, size=1)[0]\n",
    "    else:\n",
    "        return (a+b)/2 + np.sqrt((delta_t/4))*z\n",
    "    \n",
    "\n",
    "def brownian_bridge(w0, w1, n):\n",
    "\n",
    "\n",
    "    df_wt = pd.DataFrame(columns=['W_t'], index=np.arange(0, 1+1/(2**n), 1/(2**n)))\n",
    "\n",
    "    df_wt.iloc[0] = w0\n",
    "    df_wt.iloc[-1] = w1\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        \n",
    "        for j in ((1/2)**i)*np.arange(1, 2**(i)+1, 2):\n",
    "\n",
    "            df_wt.loc[j] = fill_bridge(a=df_wt.loc[j-(1/2)**i], b=df_wt.loc[j+(1/2)**i], delta_t=(1/2)**(i-1))\n",
    "\n",
    "    return df_wt\n",
    "\n",
    "\n",
    "def call_price(s0, strike_E, expiry_T, sigma, risk_free, n_periods, n_simulations):\n",
    "\n",
    "    #Returns the call price and the PV of the payoff of each simulation, using the Euler-Maruyama scheme.\n",
    "\n",
    "    prices_path = euler_maruyama(s0, expiry_T, sigma, risk_free, n_periods, n_simulations)\n",
    "    payoff = np.maximum(prices_path.iloc[-1]-strike_E,0)\n",
    "    \n",
    "    return [np.exp(-risk_free*(expiry_T))*np.mean(payoff),\n",
    "            pd.Series(payoff*np.exp(-risk_free*(expiry_T)))]\n",
    "\n",
    "\n",
    "def call_price_milstein(s0, strike_E, expiry_T, sigma, risk_free, n_periods, n_simulations):\n",
    "\n",
    "    #Returns the call price and the PV of the payoff of each simulation, using the Milstein scheme.\n",
    "\n",
    "    prices_path =  milstein(s0, expiry_T, sigma, risk_free, n_periods, n_simulations)\n",
    "    payoff = np.maximum(prices_path.iloc[-1]-strike_E,0)\n",
    "    \n",
    "    return [np.exp(-risk_free*(expiry_T))*np.mean(payoff),\n",
    "            pd.Series(payoff*np.exp(-risk_free*(expiry_T)))]\n",
    "\n",
    "\n",
    "def call_price_anti(s0, strike_E, expiry_T, sigma, risk_free, n_periods, n_simulations):\n",
    "\n",
    "    #Returns the call price and the PV of the payoff of each simulation, using Antithetics variables.\n",
    "\n",
    "    prices_plus, prices_minus = antithetic_variables(s0, expiry_T, sigma, risk_free, n_periods, n_simulations)\n",
    "    payoff = .5*(np.maximum(prices_plus.iloc[-1]-strike_E,0)+np.maximum(prices_minus.iloc[-1]-strike_E,0))\n",
    "    \n",
    "    return [np.exp(-risk_free*(expiry_T))*np.mean(payoff),\n",
    "            pd.Series(payoff*np.exp(-risk_free*(expiry_T)))]\n",
    "\n",
    "\n",
    "def call_price_sobol(s0, strike_E, expiry_T, sigma, risk_free, n_periods, n_simulations):\n",
    "\n",
    "    #Returns the call price and the PV of the payoff of each simulation, using prices paths generated by Sobol sequences.\n",
    "\n",
    "    prices_path = sobol_prices(s0, expiry_T, sigma, risk_free, n_periods, n_simulations)\n",
    "    payoff = np.maximum(prices_path.iloc[-1]-strike_E,0)\n",
    "\n",
    "    return [np.exp(-risk_free*(expiry_T))*np.mean(payoff),\n",
    "            payoff*np.exp(-risk_free*(expiry_T))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Volatility Arb with improved GBM and Monte-Carlo ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider improvements to GBM asset evolution (Euler-Maruyana/Milstein schemes). <br> Optionally, can consider modelling asset with jumps, eg, Merton jump diffusion, without going into stochastic volatility, eg Heston-Nandi. <br> Variance Gamma is also relevant but suited for single-name assets with extreme movements.\n",
    " - consider MC variance reduction techniques, such as antithetic variates; <br>\n",
    " - best practice is low discrepancy sequences, eg Sobol with the Brownian bridge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Constants\n",
    "S = 50\n",
    "K = 55\n",
    "sigma = .30\n",
    "r = .1\n",
    "T = 360/360\n",
    "\n",
    "# Calculate BS price\n",
    "bs_price = black_scholes(S, K, sigma, T, r)\n",
    "\n",
    "df_mc_errors = pd.DataFrame(columns=['Euler_Maruyama', 'Milstein', 'Antithetic', 'Sobol_Brownian_Bridge'])\n",
    "df_mc_variances = pd.DataFrame(columns=['Euler_Maruyama', 'Milstein', 'Antithetic', 'Sobol_Brownian_Bridge'])\n",
    "\n",
    "n_simulations = 2**18\n",
    "n_periods = 2**9\n",
    "\n",
    "dict_params = {'s0': S,\n",
    "               'strike_E': K,\n",
    "               'expiry_T': T, \n",
    "               'sigma': sigma, \n",
    "               'risk_free': r, \n",
    "               'n_periods': n_periods, \n",
    "               'n_simulations': int(n_simulations)}\n",
    "\n",
    "\n",
    "# Calculate BS price\n",
    "euler_maruyama_option_prices = pd.Series(call_price(**dict_params)[1])\n",
    "milstein_option_prices = pd.Series(call_price_milstein(**dict_params)[1])\n",
    "antithetic_option_prices = pd.Series(call_price_anti(**dict_params)[1])\n",
    "sobol_option_prices = call_price_sobol(**dict_params)[1]\n",
    "\n",
    "df_mc_errors['Euler_Maruyama'] = bs_price - euler_maruyama_option_prices.expanding().mean()\n",
    "df_mc_errors['Milstein'] = bs_price - milstein_option_prices.expanding().mean()\n",
    "df_mc_errors['Antithetic'] = bs_price - antithetic_option_prices.expanding().mean()\n",
    "df_mc_errors['Sobol_Brownian_Bridge'] = bs_price - sobol_option_prices.expanding().mean()\n",
    "\n",
    "str_latex = r\"\"\"\\begin{aligned}\n",
    "& \\text European\\ Call\\ Parameters\\\\\n",
    "&\\begin{array}{|l|r|r|r|r|r|r|}\n",
    "\\hline \\hline \\text { Parameter } & \\text Stock\\ Price & \\text Strike\\ Price & \\text Implied\\ Volatility & \\text Risk\\ Free & Expiry\\ (Years) & Black\\ -Scholes\\ Price \\\\\n",
    "\\hline \n",
    "\n",
    "Value & \n",
    "\"\"\" + f'{S:.2f}' + r\"\"\"\n",
    "& \"\"\" + f'{K:.2f}' + r\"\"\"\n",
    "& \"\"\" + f'{sigma*100}' + r'\\%' + r\"\"\"\n",
    "& \"\"\" + f'{r*100}' + r'\\%' + r\"\"\"\n",
    "& \"\"\" + f'{T}' + r\"\"\"\n",
    "& \"\"\" + f'{bs_price:.2f}' + r\"\"\"\n",
    "\n",
    "\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "\"\"\"\n",
    "display(Latex(str_latex))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 8))\n",
    "\n",
    "fig.suptitle('Convergence of Monte Carlo simulations methods ', fontsize=20)\n",
    "\n",
    "plt.ylabel('Running Average Error', fontsize=11)\n",
    "plt.xlabel('# of iterations', fontsize=11)\n",
    "\n",
    "sns.lineplot(df_mc_errors.loc[1024:], \n",
    "             ax=ax)\n",
    "\n",
    "plt.axhline(y=0.0, color='black', linestyle='--')\n",
    "\n",
    "ax.set_xscale('log', base=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "str_latex = r\"\"\"\\begin{aligned}\n",
    "& \\text {Error ($10^{-3}$) after $n$ iterations}\\\\\n",
    "&\\begin{array}{|l|r|r|r|}\n",
    "\\hline \\hline \\text { Method } & \\text {$n = 2^{12}$} & \\text {$n = 2^{15}$} & \\text {$n = 2^{18}$} \\\\\n",
    "\\hline \n",
    "\n",
    "Euler \\ Maruyama & \n",
    "\"\"\" + f'{np.round(df_mc_errors.loc[2**12-1].Euler_Maruyama*1e3, 2):,}' + r\"\"\"\n",
    "& \"\"\" + f'{np.round(df_mc_errors.loc[2**15-1].Euler_Maruyama*1e3, 2):,}' + r\"\"\"\n",
    "& \"\"\" + f'{np.round(df_mc_errors.loc[2**18-1].Euler_Maruyama*1e3, 2):,}' + r\"\"\"\n",
    "\n",
    "\\\\Milstein & \n",
    "\"\"\" + f'{np.round(df_mc_errors.loc[2**12-1].Milstein*1e3, 2):,}' + r\"\"\"\n",
    "& \"\"\" + f'{np.round(df_mc_errors.loc[2**15-1].Milstein*1e3, 2):,}' + r\"\"\"\n",
    "& \"\"\" + f'{np.round(df_mc_errors.loc[2**18-1].Milstein*1e3, 2):,}' + r\"\"\"\n",
    "\n",
    "\\\\Antithetic & \n",
    "\"\"\" + f'{np.round(df_mc_errors.loc[2**12-1].Antithetic*1e3, 2):,}' + r\"\"\"\n",
    "& \"\"\" + f'{np.round(df_mc_errors.loc[2**15-1].Antithetic*1e3, 2):,}' + r\"\"\"\n",
    "& \"\"\" + f'{np.round(df_mc_errors.loc[2**18-1].Antithetic*1e3, 2):,}' + r\"\"\"\n",
    "\n",
    "\\\\Sobol \\ (Brownian \\ Bridge) & \n",
    "\"\"\" + f'{np.round(df_mc_errors.loc[2**12-1].Sobol_Brownian_Bridge*1e3, 2):,}' + r\"\"\"\n",
    "& \"\"\" + f'{np.round(df_mc_errors.loc[2**15-1].Sobol_Brownian_Bridge*1e3, 2):,}' + r\"\"\"\n",
    "& \"\"\" + f'{np.round(df_mc_errors.loc[2**18-1].Sobol_Brownian_Bridge*1e3, 2):,}' + r\"\"\"\n",
    "\n",
    "\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "\"\"\"\n",
    "\n",
    "Latex(str_latex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of Sobol sequences with Brownian Bridge greatly improves the convergence of Monte Carlo calculation by orders of magnitude . <br>\n",
    "This is caused by its low-discrepancy properties that generates random numbers more evenly distributed; <br>\n",
    "<br>\n",
    "The use of antithetic variables decreses the variance of results without the necessity of further generation of random numbers <br>\n",
    "This is because it ensures symmetry on returns distrbution, forcing the distribution towards the central value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Under the condition of known future realised volatility $ V_{a} > V_{i} $ , analytically and with Monte-Carlo confirm the items below. <br> Report with both, complete mathematical workings to fold $ P\\&L_{t} $ and simulations of $ P\\&L_{t} $.\n",
    " - confirm actual volatility hedging leads to the known total $ P\\&L $;\n",
    " - confirm and demonstrate implied volatility hedging leads to uncertain total, path-dependent $ P\\&L $, and characterise on which parameters/Greeks it depends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hedging with Actual Volatility\n",
    "By buying a call option with understaded implied volatility and replicating the call with actual volatility on the opposite side trought a delta hedged portfolio, \\\n",
    "the excess return converges to the difference of option values measured with the implied volatility and measured with actual volatility.\n",
    "\n",
    "This can be shown by marking both positions to market and following its evolutions\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text {Total position at time $ t $ and $ t+dt $}\\\\\n",
    "&\\begin{array}{|c|c|c|c|}\n",
    "\\hline \\hline \\text { Instrument } & \\text { Value at t} & \\text { Value at t + dt } & \\text { Profit from t to t+dt } \\\\\n",
    "\\hline Call & V_{i} & V_{i}+dV_{i} & dV_{i} \\\\\n",
    "Stock \\ (short) & -\\Delta_{a}S & -\\Delta_{a}(S + dS) & -\\Delta_{a}dS\\\\\n",
    "Cash & (-V_{i} + \\Delta_{a}S) & (-V_{i} + \\Delta_{a}S)(1+r \\ dt)& r(-V_{i} + \\Delta_{a}S)dt\\\\\n",
    "\\hline\n",
    "Total & 0 & dV_{i} - \\Delta_{a}dS - r(V_{i} - \\Delta_{a}S)dt & dV_{i} - \\Delta_{a}dS - r(V_{i} - \\Delta_{a}S)dt \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Beacuse the option would be correctly value at $V_{a}$ then we have\n",
    "\n",
    "\n",
    "$$  dV_{a} - \\Delta_{a}dS  - r(V_{a} - \\Delta_{a}S) dt = 0$$\n",
    "\n",
    "Subtracing the previous expression from the profit we have\n",
    "\n",
    "$$  dV_{i} - dV_{a} + r(V_{a} - \\Delta_{a}S) - r(V_{i} - \\Delta_{a}S) dt $$\n",
    "\n",
    "All $dS$ terms vanish beacuse the portfolio is delta hedged\n",
    "\n",
    "$$ = dV_{i} - dV_{a} + r(V_{a} - V_{i})dt $$\n",
    "\n",
    "using the Integrating Factor $e^{-rt}$\n",
    "$$ = e^{rt} d(e^{-rt}(V_{i}-V_{a})) $$ \n",
    "\n",
    "PV-ing that increment of profit to $t_{0}$ gives\n",
    "\n",
    "$$ = e^{-r(t-t_{0})}e^{rt} d(e^{-rt}(V_{i}-V_{a})) = e^{rt_{0}}d(e^{-rt}(V_{i}-V_{a}))$$ \n",
    "\n",
    "Integrating all the increments at PV:\n",
    "\n",
    "$$ e^{rt_{0}}\\int_{t_{0}}^{T}d(e^{-rt}(V_{i}-V_{a})) = V_{a}-V_{i} $$\n",
    "\n",
    "Expected Total Profit at FV:\n",
    "\n",
    "$$ e^{rT}(V_{a}-V_{i}) $$\n",
    "\n",
    "Now, we implement this in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_hedged_ptf(stock_path, K, T, sigma, r):\n",
    "\n",
    "    # Given an asset price evolution, Strike, maturity, volatility and risk-free rate, \n",
    "    # this functions returns the evolution of the position of a delta hedged portfolio \n",
    "\n",
    "    df_hedged_ptf = pd.DataFrame(columns=['Cash_Position', 'Cash_Rebalance', \n",
    "                                          'Daily_Interest', '#_Stocks', \n",
    "                                          'Ptf_Stock', 'Ptf_Total'],\n",
    "                                 index=range(0,len(stock_path)))\n",
    "    \n",
    "    \n",
    "    s0 = stock_path[0]\n",
    "    n_periods = len(stock_path)\n",
    "    \n",
    "    df_hedged_ptf['Cash_Position'] = [0.0]*len(df_hedged_ptf)\n",
    "    df_hedged_ptf['Cash_Rebalance'] = [0.0]*len(df_hedged_ptf)\n",
    "    df_hedged_ptf['Daily_Interest'] = [0.0]*len(df_hedged_ptf)\n",
    "\n",
    "    df_hedged_ptf['#_Stocks'] = [bs_delta(S=s, K=K, sigma=sigma, T=T*(1-t/n_periods), r=r) for t,s in zip(df_hedged_ptf.index, stock_path)]\n",
    "    df_hedged_ptf.loc[0, 'Cash_Position'] = -df_hedged_ptf.loc[0, '#_Stocks']*s0\n",
    "\n",
    "    df_hedged_ptf['Ptf_Stock'] = df_hedged_ptf['#_Stocks']*stock_path\n",
    "    df_hedged_ptf['Cash_Rebalance'] = -df_hedged_ptf['#_Stocks'].diff().fillna(0)*stock_path\n",
    "\n",
    "    for i in df_hedged_ptf.index[1:]:\n",
    "\n",
    "        df_hedged_ptf.loc[i, 'Daily_Interest'] = df_hedged_ptf.loc[i-1, 'Cash_Position']*r*T/n_periods\n",
    "        df_hedged_ptf.loc[i, 'Cash_Position'] = df_hedged_ptf.loc[i-1, 'Cash_Position'] + df_hedged_ptf.loc[i, 'Cash_Rebalance'] + df_hedged_ptf.loc[i, 'Daily_Interest']\n",
    "\n",
    "    df_hedged_ptf['Ptf_Total'] = df_hedged_ptf['Cash_Position'] + df_hedged_ptf['Ptf_Stock']\n",
    "\n",
    "    return df_hedged_ptf\n",
    "\n",
    "\n",
    "def call_ptf(stock_path, K, T,sigma, r):\n",
    "\n",
    "    # Given an asset price evolution, Strike, maturity, volatility and risk-free rate, \n",
    "    # this functions returns the evolution of the position of a portfolio with a single call.\n",
    "\n",
    "    s0 = stock_path[0]\n",
    "    n_periods = len(stock_path)\n",
    "\n",
    "    V_i = black_scholes(S=s0, K=K, sigma=sigma, T=T, r=r)\n",
    "\n",
    "    df_call_ptf = pd.DataFrame(columns=['Call_Price', 'Call_Carry', 'Call_PnL'],\n",
    "                               index=range(0,len(stock_path)))\n",
    "\n",
    "    df_call_ptf['Call_Price'] = [black_scholes(S=s, K=K, sigma=sigma, T=T*(1-t/n_periods), r=r) for t,s in zip(df_call_ptf.index, stock_path)]\n",
    "    df_call_ptf['Call_Carry'] = [V_i*(1-np.exp(r*T*t/n_periods)) for t in df_call_ptf.index]\n",
    "    df_call_ptf['Call_PnL'] = df_call_ptf['Call_Price'] - df_call_ptf.loc[0,'Call_Price'] + df_call_ptf['Call_Carry']\n",
    "    \n",
    "    df_call_ptf['Call_Delta'] = [bs_delta(S=s, K=K, sigma=sigma, T=T*(1-t/n_periods), r=r) for t,s in zip(df_call_ptf.index, stock_path)]\n",
    "    df_call_ptf['Call_Gamma'] = [bs_gamma(S=s, K=K, sigma=sigma, T=T*(1-t/n_periods), r=r) for t,s in zip(df_call_ptf.index, stock_path)]\n",
    "    df_call_ptf['Call_Theta'] = [bs_theta(S=s, K=K, sigma=sigma, T=T*(1-t/n_periods), r=r) for t,s in zip(df_call_ptf.index, stock_path)]\n",
    "    df_call_ptf['Call_Rho'] = [bs_rho(S=s, K=K, sigma=sigma, T=T*(1-t/n_periods), r=r) for t,s in zip(df_call_ptf.index, stock_path)]\n",
    "    df_call_ptf['Call_Vega'] = [bs_vega(S=s, K=K, sigma=sigma, T=T*(1-t/n_periods), r=r) for t,s in zip(df_call_ptf.index, stock_path)]\n",
    "\n",
    "    df_call_ptf['PnL_Delta'] = (df_call_ptf['Call_Delta'].shift(1)*stock_path.diff()).cumsum()\n",
    "    df_call_ptf['PnL_Gamma'] = (df_call_ptf['Call_Gamma'].shift(1)*np.square(stock_path.diff())).cumsum()/2\n",
    "    df_call_ptf['PnL_Theta'] = (df_call_ptf['Call_Theta'].shift(1)*T/n_periods).cumsum()\n",
    "    df_call_ptf['PnL_Carry'] = df_call_ptf['Call_Carry']\n",
    "\n",
    "    df_call_ptf['PnL_Greeks'] = df_call_ptf[['PnL_Delta','PnL_Gamma','PnL_Theta','PnL_Carry']].sum(axis=1)\n",
    "\n",
    "    return df_call_ptf\n",
    "\n",
    "    \n",
    "S = 50\n",
    "K = 55\n",
    "sigma_i = .30\n",
    "sigma_a = .40\n",
    "\n",
    "r = .1\n",
    "T = 1.0 #in years\n",
    "\n",
    "V_i = black_scholes(S, K, sigma_i, T, r)\n",
    "V_a = black_scholes(S, K, sigma_a, T, r)\n",
    "\n",
    "n_periods = 10000\n",
    "\n",
    "df_price_paths = euler_maruyama(s0=S, expiry_T=T, sigma=sigma_a, return_rate=r, n_periods=n_periods, n_simulations=10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 8))\n",
    "\n",
    "for i in df_price_paths.columns:\n",
    "\n",
    "    stock_path = df_price_paths.loc[:, i]\n",
    "\n",
    "    df_call = call_ptf(stock_path = stock_path, K=K, T=T,sigma=sigma_i, r=r).Call_PnL\n",
    "    df_ptf = delta_hedged_ptf(stock_path = stock_path, K=K,T=T, sigma=sigma_a, r=r).Ptf_Total\n",
    "\n",
    "    sns.lineplot(y=(df_call - df_ptf).values,\n",
    "                 x=(df_call - df_ptf).index,\n",
    "                 ax=ax)\n",
    "\n",
    "plt.axhline(y=np.exp(r*T)*(V_a-V_i), color='black', linestyle='--')\n",
    "plt.xlabel('# of iterations', fontsize=11)\n",
    "\n",
    "ax.text(x = 0, \n",
    "        y = V_a - V_i, \n",
    "        s = r'$ e^{rT}(V_{a}-V_{i}) = $' + str(np.round(np.exp(r*T)*(V_a-V_i),2)), \n",
    "        bbox=props,\n",
    "        fontsize=17)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulations shows that, as expected, the $P\\&L$ converges to the Future Value of $V_{a}-V_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hedging with Implied Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1.0\n",
    "\n",
    "V_i = black_scholes(S, K, sigma_i, T, r)\n",
    "V_a = black_scholes(S, K, sigma_a, T, r)\n",
    "\n",
    "#n_periods = 10000\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 8))\n",
    "\n",
    "for i in df_price_paths.columns[0:]:\n",
    "\n",
    "    stock_path = df_price_paths.loc[:, i]\n",
    "\n",
    "    df_call = call_ptf(stock_path = stock_path, K=K, T=T,sigma=sigma_i, r=r).Call_PnL\n",
    "    df_ptf = delta_hedged_ptf(stock_path = stock_path, K=K,T=T, sigma=sigma_i, r=r).Ptf_Total\n",
    "\n",
    "    sns.lineplot(x=(df_call - df_ptf).index,\n",
    "                 y=(df_call - df_ptf).values,\n",
    "                 ax=ax)\n",
    "\n",
    "plt.xlabel('# of iterations', fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The simulations shows that hedging with implied volatility lead to unknown $P\\&L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Think of additional analysis: consider how $ P\\&L $ decomposes in terms of Greeks. <br> What is the impact of time-dependent Gamma $ \\Gamma_{t} $? <br> What about $ r^2\n",
    "− \\sigma_{imp}\\delta t $? <br> Consider findings\n",
    "from Part II MVD modelling, what are the implications of hedging with the smaller delta?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total profit can be expressed as:\n",
    "\n",
    "$$ \\frac{1}{2}\\int_{t_{0}}^{T}e^{-r(t-t_{0})}\\ S^2_{t}\\ \\Gamma^{i}_{t}\\ (r^2_{t}-\\sigma_{t,imp}^2t)dt $$\n",
    "\n",
    "Which we will calculate for the same stock paths generated in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedge_with_implied(stock_path, gamma, sigma_a, sigma_i):\n",
    "    \n",
    "    n_periods = len(stock_path)\n",
    "\n",
    "    realized_vol = np.sqrt((np.log(stock_path/stock_path.shift(1))**2).cumsum())/np.sqrt((np.array(range(0, n_periods))+1)/n_periods)\n",
    "    realized_sigma = realized_vol**2\n",
    "    discount_factors = np.exp(-r*((np.array(range(0, n_periods))+1)/n_periods))\n",
    "\n",
    "\n",
    "    integral = (discount_factors*((stock_path)**2)*gamma*(realized_sigma-sigma_i**2)/n_periods).cumsum()\n",
    "\n",
    "    return .5*integral\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 8))\n",
    "\n",
    "for i in df_price_paths.columns[0:]:\n",
    "\n",
    "    stock_path = df_price_paths.loc[:, i]\n",
    "\n",
    "    df_call = call_ptf(stock_path = stock_path, K=K, T=T,sigma=sigma_i, r=r)\n",
    "    df_ptf = delta_hedged_ptf(stock_path = stock_path, K=K,T=T, sigma=sigma_i, r=r)\n",
    "\n",
    "    sns.lineplot((hedge_with_implied(stock_path, df_call.Call_Gamma, sigma_a, sigma_i)),\n",
    "                 ax=ax)\n",
    "    \n",
    "ax.text(x = 0, \n",
    "        y = V_a - V_i, \n",
    "        s = r'Gamma - Theta = $\\frac{1}{2}\\int_{t_{0}}^{T}e^{-r(t-t_{0})}\\ S^2_{t}\\ \\Gamma^{i}_{t}\\ (r^2_{t}-\\sigma_{t,imp}^2t)dt$',\n",
    "        bbox=props,\n",
    "        fontsize=17)\n",
    "\n",
    "plt.xlabel('# of iterations', fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the results match with the results obtained on the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Minimum Variance Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. begin with sorting your IV data – or each trading day, you will need BS option price as\n",
    "implied vol percentage, delta, and vega: ($ V_{t} $,$ \\delta_{bs} $,$ \\nu_{bs} $). <br> \n",
    "The term structure for option expiry $ 1M, 3M, 6M, 9M, 12M $, weekly expiries not necessary. <br> \n",
    "Key choice to make here, if\n",
    "you are going to study Delta for out of the money call strikes, in addition to about ATM\n",
    "buckets $ 0.45 < \\delta_{bs} <0.55 $ – each strike means a separate a,b,c history for each expiry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we read and parse calls and puts options data from OptionDX.com; <br>\n",
    "Only options on the SPX from January 2010 from September 2023 are used; <br>\n",
    "\n",
    "We are removing the following data:\n",
    " - Rows with <b> Null</b> values;\n",
    " - Options with expiration time below 14 days;\n",
    " - Rows with prices in holidays;\n",
    "\n",
    "For each row, the price according to the Black Scholes formula is also calculated. <br>\n",
    "All consolidated data is stored in the dataframe <b>df_option_data</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dtpyes = {'[QUOTE_UNIXTIME]': int,\n",
    "               ' [QUOTE_READTIME]' : object,\n",
    "               ' [QUOTE_DATE]': str,\n",
    "               ' [QUOTE_TIME_HOURS]': float, \n",
    "               ' [UNDERLYING_LAST]': float,\n",
    "               ' [EXPIRE_DATE]': object,\n",
    "               ' [EXPIRE_UNIX]': int,\n",
    "               ' [DTE]': float,\n",
    "               ' [C_DELTA]': object,\n",
    "               ' [C_GAMMA]': object, \n",
    "               ' [C_VEGA]': object,\n",
    "               ' [C_THETA]': object, \n",
    "               ' [C_RHO]': object, \n",
    "               ' [C_IV]': object, \n",
    "               ' [C_VOLUME]':object, \n",
    "               ' [C_LAST]': object,\n",
    "               ' [C_SIZE]': object,\n",
    "               ' [C_BID]': object, \n",
    "               ' [C_ASK]':object, \n",
    "               ' [STRIKE]': object, \n",
    "               ' [P_BID]':object,\n",
    "               ' [P_ASK]':object,  \n",
    "               ' [P_SIZE]':object, \n",
    "               ' [P_LAST]':object,\n",
    "               ' [P_DELTA]': object,\n",
    "               ' [P_GAMMA]': object,\n",
    "               ' [P_VEGA]': object, \n",
    "               ' [P_THETA]': object, \n",
    "               ' [P_RHO]':object,\n",
    "               ' [P_IV]':object,\n",
    "               ' [P_VOLUME]': object,\n",
    "               ' [STRIKE_DISTANCE]': float,\n",
    "               ' [STRIKE_DISTANCE_PCT]': float}\n",
    "\n",
    "csv_files = sorted(os.listdir('spx_eod_data'))\n",
    "\n",
    "df_option_data = pd.DataFrame()\n",
    "\n",
    "for file in csv_files:\n",
    "\n",
    "    df_option_data = pd.concat([df_option_data,\n",
    "                                pd.read_csv(filepath_or_buffer=f'spx_eod_data/{file}', \n",
    "                                            dtype=dict_dtpyes)\n",
    "                                ])\n",
    "\n",
    "\n",
    "#Parse Data\n",
    "df_option_data.columns = [x.strip().replace('[','').replace(']','') for x in df_option_data.columns]\n",
    "df_option_data['QUOTE_DATE'] = [datetime.strptime(x.strip(),'%Y-%m-%d') for x in df_option_data.QUOTE_DATE]\n",
    "df_option_data['EXPIRE_DATE'] = [datetime.strptime(x.strip(),'%Y-%m-%d') for x in df_option_data.EXPIRE_DATE]\n",
    "\n",
    "df_option_data = df_option_data.replace(' ', np.nan)\n",
    "df_option_data = df_option_data[~df_option_data.isna().any(axis=1)]\n",
    "df_option_data = df_option_data.loc[df_option_data.DTE > 14]\n",
    "\n",
    "for col in ['C_DELTA', 'C_GAMMA', 'C_VEGA', 'C_THETA', 'C_RHO', 'C_IV', 'C_VOLUME', 'C_LAST', 'C_BID', 'C_ASK',\n",
    "            'STRIKE', 'P_BID', 'P_ASK', 'P_LAST', 'P_DELTA', 'P_GAMMA', 'P_VEGA', 'P_THETA', 'P_RHO', 'P_IV', 'P_VOLUME', \n",
    "            'STRIKE_DISTANCE', 'STRIKE_DISTANCE_PCT']:\n",
    "    \n",
    "    df_option_data[col] = [float(x) for x in df_option_data[col]]\n",
    "\n",
    "df_option_data['C_MID'] = (df_option_data['C_ASK']+df_option_data['C_BID'])/2\n",
    "df_option_data['P_MID'] = (df_option_data['P_ASK']+df_option_data['P_BID'])/2\n",
    "\n",
    "df_option_data = df_option_data.reset_index(drop=True)\n",
    "\n",
    "#Obtain Black Scholes Prices\n",
    "df_option_data['C_BS'] = [black_scholes(S=S,K=K,sigma=sigma,T=T/360,r=0) \n",
    "                          for S,K,sigma,T in zip(df_option_data['UNDERLYING_LAST'],\n",
    "                                                 df_option_data['STRIKE'],\n",
    "                                                 df_option_data['C_IV'], \n",
    "                                                 df_option_data['DTE'])]\n",
    "\n",
    "df_option_data['P_BS'] = [black_scholes(S=S,K=K,sigma=sigma,T=T/360,r=0, option_type='put') \n",
    "                          for S,K,sigma,T in zip(df_option_data['UNDERLYING_LAST'],\n",
    "                                                 df_option_data['STRIKE'],\n",
    "                                                 df_option_data['C_IV'], \n",
    "                                                 df_option_data['DTE'])]\n",
    "\n",
    "#Gather Holidays\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "\n",
    "holidays = cal.holidays(start=df_option_data.QUOTE_DATE.min(), \n",
    "                        end=df_option_data.QUOTE_DATE.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. compute your dependent variable and run the fitting on $ \\delta_{bs} $,$ \\delta^2_{bs} $. <br>Dependent side based\n",
    "on daily option price changes $ \\Delta V_{t} $, and you will need $( \\Delta S_{t} , S_{t}) $ as well as Greeks noted\n",
    "above. <br> The exact data columns will depend on how you organise regression or do SLSQP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we extract only data for calls and remove the follwing rows: - deep ITM (absolute delta above 95%) \n",
    "- deep OTM (absolute delta below 5%)\n",
    "- outlier <b>C_LAST/ P_LAST</b> prices compared to Black Scholes prices\n",
    "- calls/puts with no volume dring the day\n",
    "\n",
    "Each price is label on a Delta bucket (from .1 to .9) and a maturity bucket ('1M', '3M', '6M', '9M', '12M') <br>\n",
    "The resulting data is stored in <b>df_calls</b>/<b>df_puts</b>\n",
    "\n",
    "The next step is to compare the evolution of each contract from one day to the next day. <br>\n",
    "To do this, each option is specified by <b>EXPIRE_UNIX</b> and <b>STRIKE</b> and its data is compared with the data on the previous trading day. <br>\n",
    "On each comparison the following columns are removed: <br>\n",
    "\n",
    " - Different delta buckets between days;\n",
    " - Different expiry buckets between days;\n",
    " - Days with very tiny underlying price movements;\n",
    "\n",
    "\n",
    "On each comparison the following columns are calculated:\n",
    "\n",
    " - <b>delta_price</b>: variation of last price\n",
    " - <b>delta_IV</b>: variation of Implied volatility \n",
    " - <b>delta_S</b>: variation of Underlying asset\n",
    " - <b>y</b>: variable used for future fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calls = df_option_data.set_index(['QUOTE_DATE','EXPIRE_UNIX','STRIKE'])[['UNDERLYING_LAST', 'DTE', 'C_DELTA', 'C_VEGA', 'C_IV', 'C_LAST','C_MID','C_BS','C_VOLUME']]\n",
    "df_calls['QUOTE_DATE'] = df_calls.index.get_level_values(0)\n",
    "\n",
    "df_calls = df_calls[(df_calls.C_DELTA < .95)&(df_calls.C_DELTA > .05)]\n",
    "df_calls = df_calls[~df_calls.index.get_level_values(0).isin(holidays)].copy()\n",
    "df_calls = df_calls[df_calls.C_VOLUME!=0].copy()\n",
    "\n",
    "df_calls['BS_Delta_Bucket'] = [round(x*10)/10 for x in df_calls.C_DELTA]\n",
    "\n",
    "df_calls['Expiry_Bucket'] = pd.cut(df_calls.DTE,[14, 30, 91, 182, 365, \n",
    "                                                 df_calls.DTE.max()],\n",
    "                                        labels=['1M', '3M', '6M', '9M', '12M'])\n",
    "\n",
    "# Eliminate outliers prices compared to Black Scholes price\n",
    "\n",
    "q75, q25 = np.percentile((df_calls['C_LAST']-df_calls['C_BS']).values,\n",
    "                          [75,25])\n",
    "intr_qr = q75-q25\n",
    "\n",
    "cut_max = q75+(1.5*intr_qr)\n",
    "cut_min = q25-(1.5*intr_qr)\n",
    "\n",
    "df_calls = df_calls[(df_calls['C_LAST']-df_calls['C_BS'] > cut_min) \n",
    "                    & (df_calls['C_LAST']-df_calls['C_BS'] < cut_max)].copy()\n",
    "\n",
    "quote_dates = sorted(df_calls.index.get_level_values(0).unique())\n",
    "\n",
    "df_calls_diff = df_calls.loc[quote_dates[0]].merge(\n",
    "    df_calls.loc[quote_dates[1]], \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    suffixes=('_t1','_t0'), \n",
    "    how='inner')\n",
    "\n",
    "\n",
    "for i in range(len(quote_dates[2:])):\n",
    "    df_calls_diff = pd.concat([df_calls_diff,\n",
    "                               df_calls.loc[quote_dates[i+1]].merge(\n",
    "                                   df_calls.loc[quote_dates[i+2]], \n",
    "                                   left_index=True,\n",
    "                                   right_index=True,\n",
    "                                   suffixes=('_t1','_t0'),\n",
    "                                   how='inner')],\n",
    "                     axis=0)\n",
    "\n",
    "#df_calls_diff = df_calls_diff[df_calls_diff.BS_Delta_Bucket_t1 == df_calls_diff.BS_Delta_Bucket_t0].copy()\n",
    "df_calls_diff = df_calls_diff[df_calls_diff.Expiry_Bucket_t1 == df_calls_diff.Expiry_Bucket_t0].copy()\n",
    "\n",
    "df_calls_diff = df_calls_diff[df_calls_diff['C_LAST_t1']-df_calls_diff['C_LAST_t0'] != 0]\n",
    "    \n",
    "df_calls_diff['delta_price'] = df_calls_diff['C_LAST_t0'] - df_calls_diff['C_LAST_t1']\n",
    "df_calls_diff['delta_S'] = df_calls_diff['UNDERLYING_LAST_t0'] - df_calls_diff['UNDERLYING_LAST_t1']\n",
    "\n",
    "df_calls_diff['delta_price_perc'] = (df_calls_diff['C_LAST_t0'] - df_calls_diff['C_LAST_t1'])/df_calls_diff['C_LAST_t1']\n",
    "df_calls_diff['delta_S_perc'] = (df_calls_diff['UNDERLYING_LAST_t0'] - df_calls_diff['UNDERLYING_LAST_t1'])/df_calls_diff['UNDERLYING_LAST_t1']\n",
    "\n",
    "df_calls_diff['delta_IV'] = df_calls_diff['C_IV_t0']-df_calls_diff['C_IV_t1']\n",
    "\n",
    "df_calls_diff = df_calls_diff.rename({'BS_Delta_Bucket_t1':'BS_Delta_Bucket',\n",
    "                                      'Expiry_Bucket_t1':'Expiry_Bucket'\n",
    "                                      },axis=1)\n",
    "\n",
    "df_calls_diff = df_calls_diff.drop(['BS_Delta_Bucket_t0','Expiry_Bucket_t0'])\n",
    "\n",
    "df_calls_diff = df_calls_diff[abs(df_calls_diff.delta_S_perc) > 1e-4].copy()\n",
    "df_calls_diff = df_calls_diff.set_index(['QUOTE_DATE_t1','BS_Delta_Bucket','Expiry_Bucket'])\n",
    "\n",
    "df_calls_diff['delta_IV'] = df_calls_diff['C_IV_t0'] - df_calls_diff['C_IV_t1']\n",
    "\n",
    "df_calls_diff['y'] = (df_calls_diff['delta_price']/df_calls_diff['delta_S'] - df_calls_diff['C_DELTA_t1'])*df_calls_diff['UNDERLYING_LAST_t1']*np.sqrt(df_calls_diff['DTE_t1']/360)/(df_calls_diff['C_VEGA_t1']*100)\n",
    "df_calls_diff['y_perc'] = (df_calls_diff['delta_price_perc']/df_calls_diff['delta_S_perc']-df_calls_diff['C_DELTA_t1'])*np.sqrt(df_calls_diff['DTE_t1']/360)/(df_calls_diff['C_VEGA_t1']*100)\n",
    "\n",
    "df_calls_diff['delta_S_perc/sqrt_t'] = df_calls_diff['delta_S_perc']/np.sqrt(df_calls_diff['DTE_t0']/360)\n",
    "\n",
    "# Eliminate outliers on y\n",
    "q75, q25 = np.percentile(df_calls_diff.y, [75,25])\n",
    "intr_qr = q75-q25\n",
    "\n",
    "cut_max = q75+(1.5*intr_qr)\n",
    "cut_min = q25-(1.5*intr_qr)\n",
    "\n",
    "df_calls_diff = df_calls_diff[(df_calls_diff['y'] > cut_min) & (df_calls_diff['y'] < cut_max)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puts = df_option_data.set_index(['QUOTE_DATE','EXPIRE_UNIX','STRIKE'])[['UNDERLYING_LAST', 'DTE', 'P_DELTA', 'P_VEGA', 'P_IV', 'P_LAST','P_MID','P_BS','P_VOLUME']]\n",
    "df_puts['QUOTE_DATE'] = df_puts.index.get_level_values(0)\n",
    "\n",
    "df_puts = df_puts[(df_puts.P_DELTA > -.95)&(df_puts.P_DELTA < -.05)]\n",
    "df_puts = df_puts[~df_puts.index.get_level_values(0).isin(holidays)].copy()\n",
    "df_puts = df_puts[df_puts.P_VOLUME!=0].copy()\n",
    "\n",
    "df_puts['BS_Delta_Bucket'] = [round(x*10)/10 for x in df_puts.P_DELTA]\n",
    "\n",
    "df_puts['Expiry_Bucket'] = pd.cut(df_puts.DTE,[14, 30, 91, 182, 365, \n",
    "                                                 df_puts.DTE.max()],\n",
    "                                        labels=['1M', '3M', '6M', '9M', '12M'])\n",
    "\n",
    "# Eliminate outliers prices compared to Black Scholes price\n",
    "\n",
    "q75, q25 = np.percentile((df_puts['P_LAST']-df_puts['P_BS']).values,\n",
    "                          [75,25])\n",
    "intr_qr = q75-q25\n",
    "\n",
    "cut_max = q75+(1.5*intr_qr)\n",
    "cut_min = q25-(1.5*intr_qr)\n",
    "\n",
    "df_puts = df_puts[(df_puts['P_LAST']-df_puts['P_BS'] > cut_min) \n",
    "                    & (df_puts['P_LAST']-df_puts['P_BS'] < cut_max)].copy()\n",
    "\n",
    "quote_dates = sorted(df_puts.index.get_level_values(0).unique())\n",
    "\n",
    "df_puts_diff = df_puts.loc[quote_dates[0]].merge(\n",
    "    df_puts.loc[quote_dates[1]], \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    suffixes=('_t1','_t0'), \n",
    "    how='inner')\n",
    "\n",
    "\n",
    "for i in range(len(quote_dates[2:])):\n",
    "    df_puts_diff = pd.concat([df_puts_diff,\n",
    "                               df_puts.loc[quote_dates[i+1]].merge(\n",
    "                                   df_puts.loc[quote_dates[i+2]], \n",
    "                                   left_index=True,\n",
    "                                   right_index=True,\n",
    "                                   suffixes=('_t1','_t0'),\n",
    "                                   how='inner')],\n",
    "                     axis=0)\n",
    "\n",
    "#df_puts_diff = df_puts_diff[df_puts_diff.BS_Delta_Bucket_t1 == df_puts_diff.BS_Delta_Bucket_t0].copy()\n",
    "df_puts_diff = df_puts_diff[df_puts_diff.Expiry_Bucket_t1 == df_puts_diff.Expiry_Bucket_t0].copy()\n",
    "\n",
    "df_puts_diff = df_puts_diff[df_puts_diff['P_LAST_t1']-df_puts_diff['P_LAST_t0'] != 0]\n",
    "    \n",
    "df_puts_diff['delta_price'] = df_puts_diff['P_LAST_t0'] - df_puts_diff['P_LAST_t1']\n",
    "df_puts_diff['delta_S'] = df_puts_diff['UNDERLYING_LAST_t0'] - df_puts_diff['UNDERLYING_LAST_t1']\n",
    "\n",
    "df_puts_diff['delta_price_perc'] = (df_puts_diff['P_LAST_t0'] - df_puts_diff['P_LAST_t1'])/df_puts_diff['P_LAST_t1']\n",
    "df_puts_diff['delta_S_perc'] = (df_puts_diff['UNDERLYING_LAST_t0'] - df_puts_diff['UNDERLYING_LAST_t1'])/df_puts_diff['UNDERLYING_LAST_t1']\n",
    "\n",
    "df_puts_diff['delta_IV'] = df_puts_diff['P_IV_t0']-df_puts_diff['P_IV_t1']\n",
    "\n",
    "df_puts_diff = df_puts_diff.rename({'BS_Delta_Bucket_t1':'BS_Delta_Bucket',\n",
    "                                      'Expiry_Bucket_t1':'Expiry_Bucket'\n",
    "                                      },axis=1)\n",
    "\n",
    "df_puts_diff = df_puts_diff.drop(['BS_Delta_Bucket_t0','Expiry_Bucket_t0'])\n",
    "\n",
    "df_puts_diff = df_puts_diff[abs(df_puts_diff.delta_S_perc) > 1e-4].copy()\n",
    "df_puts_diff = df_puts_diff.set_index(['QUOTE_DATE_t1','BS_Delta_Bucket','Expiry_Bucket'])\n",
    "\n",
    "df_puts_diff['y'] = (df_puts_diff['delta_price']/df_puts_diff['delta_S'] - df_puts_diff['P_DELTA_t1'])*df_puts_diff['UNDERLYING_LAST_t1']*np.sqrt(df_puts_diff['DTE_t1']/360)/(df_puts_diff['P_VEGA_t1']*100)\n",
    "#df_puts_diff['y'] = -df_puts_diff['y']\n",
    "\n",
    "df_puts_diff['y_perc'] = (df_puts_diff['delta_price_perc']/df_puts_diff['delta_S_perc']-df_puts_diff['P_DELTA_t1'])*np.sqrt(df_puts_diff['DTE_t1']/360)/(df_puts_diff['P_VEGA_t1']*100)\n",
    "\n",
    "df_puts_diff['delta_S_perc/sqrt_t'] = df_puts_diff['delta_S_perc']/np.sqrt(df_puts_diff['DTE_t0']/360)\n",
    "df_puts_diff['delta_IV_adj'] = df_puts_diff['delta_IV']/df_puts_diff['delta_S_perc/sqrt_t']\n",
    "\n",
    "# Eliminate outliers on y\n",
    "q75, q25 = np.percentile(df_puts_diff.y, [75,25])\n",
    "intr_qr = q75-q25\n",
    "\n",
    "cut_max = q75+(1.5*intr_qr)\n",
    "cut_min = q25-(1.5*intr_qr)\n",
    "\n",
    "df_puts_diff = df_puts_diff[(df_puts_diff['y'] > cut_min) & (df_puts_diff['y'] < cut_max)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_puts_diff['y'] = (df_puts_diff['delta_price']/df_puts_diff['delta_S'] - df_puts_diff['P_DELTA_t1'])*df_puts_diff['UNDERLYING_LAST_t1']*np.sqrt(df_puts_diff['DTE_t1']/360)/(df_puts_diff['P_VEGA_t1']*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df_puts_diff.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_calls_diff.y.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puts_diff.P_DELTA_t0.iloc[0] <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puts_diff['delta_IV_adj'] = df_puts_diff['delta_IV']/df_puts_diff['delta_S_perc/sqrt_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puts_diff.loc[training_period].groupby(['Expiry_Bucket','BS_Delta_Bucket'],\n",
    "                                  observed=True).y.median().loc['1M'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puts_diff.groupby(['Expiry_Bucket','BS_Delta_Bucket'],\n",
    "                                  observed=True).y.mean().loc['3M'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puts_diff.loc[training_period].groupby(['Expiry_Bucket','BS_Delta_Bucket'],\n",
    "                                  observed=True).delta_IV_adj.mean().loc['3M'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=df_puts_diff['delta_S_perc/sqrt_t'].loc[training_period],\n",
    "            x=df_puts_diff.loc[training_period].index.get_level_values(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. parameters a,b,c can be constant for a study project, but rolling estimation itself is a calibration\n",
    "technique because for each expiry, you have time-dependent a,b,c (not 3 constants). <br>Hull White\n",
    "recipe was 3M rolling window, then shift the start date by one day (3 ×22 obs) – you can estimate\n",
    "with shorter/longer periods or shift by 5−10 days.<br> Also remember, option IVs can be simulated\n",
    "from a uniform distribution recipe.\n",
    "\n",
    "4. For model validation, look at change of a,b,c over time – we use regression as a fitting tool\n",
    "so they might not even be statistically significant. <br>Check if δMV−δBS gives an (inverted)\n",
    "parabolic shape, plot expected change in IV vs Delta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are fitting the rolling parameters for each testing window and each expiry bucket. <br>The parameters are fitted for 36 months windows (756 trading days). <br>The plot below shows the evolution of those parameters for each expiry bucket and also an aggregated set of parameters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_squared(df_option_prices):\n",
    "\n",
    "    df = df_option_prices[['delta_IV', 'delta_S_perc', 'DTE_t1']].copy()\n",
    "    df['x'] = df['delta_S_perc']/np.sqrt(df['DTE_t1']/360)\n",
    "    result = pd.Series(index=sorted(df.index.get_level_values(1).unique()))\n",
    "\n",
    "    for bs_bucket in sorted(df.index.get_level_values(1).unique()):\n",
    "        try:\n",
    "            slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(y=df.xs(bs_bucket, level='BS_Delta_Bucket').delta_IV,\n",
    "                                                                            x=df.xs(bs_bucket, level='BS_Delta_Bucket').x)\n",
    "            \n",
    "            result.loc[bs_bucket] = r_value**2\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "    return result\n",
    "    \n",
    "    \n",
    "def quadratic_fit(df_option_prices,option_type='call'):\n",
    "\n",
    "    df = df_option_prices.groupby(['Expiry_Bucket','BS_Delta_Bucket'],\n",
    "                                  observed=True).y.mean()\n",
    "\n",
    "    expiry_buckets  = df.index.get_level_values(0).unique().to_list() + ['Total']\n",
    "\n",
    "    df_result = pd.DataFrame(index=expiry_buckets,\n",
    "                             columns=['a', 'b', 'c'])\n",
    "\n",
    "    if option_type == 'call':\n",
    "\n",
    "        for expiry in expiry_buckets[:-1]:\n",
    "\n",
    "            c,b,a = np.polyfit(y=df.loc[expiry],\n",
    "                            x=df.loc[expiry].index,\n",
    "            deg=2)\n",
    "            \n",
    "            df_result.loc[expiry,['a', 'b', 'c']] = [a, b, c]\n",
    "        \n",
    "        c,b,a = np.polyfit(y=df_option_prices.groupby(['BS_Delta_Bucket'],\n",
    "                                                    observed=True).y.mean(),\n",
    "                            x=df_option_prices.groupby(['BS_Delta_Bucket'],\n",
    "                                                    observed=True).y.mean().index,\n",
    "            deg=2)\n",
    "        \n",
    "    elif option_type == 'put':\n",
    "\n",
    "        for expiry in expiry_buckets[:-1]:\n",
    "\n",
    "            c,b,a = np.polyfit(y=df.loc[expiry],\n",
    "                            x=df.loc[expiry].index+1,\n",
    "            deg=2)\n",
    "            \n",
    "            df_result.loc[expiry,['a', 'b', 'c']] = [a, b, c]\n",
    "        \n",
    "        c,b,a = np.polyfit(y=df_option_prices.groupby(['BS_Delta_Bucket'],\n",
    "                                                    observed=True).y.mean(),\n",
    "                            x=df_option_prices.groupby(['BS_Delta_Bucket'],\n",
    "                                                    observed=True).y.mean().index+1,\n",
    "            deg=2)\n",
    "\n",
    "    else:\n",
    "    \n",
    "        raise Exception('Option type not supported')\n",
    "\n",
    "\n",
    "    \n",
    "    df_result.loc['Total',['a', 'b', 'c']] = [a, b, c]\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_fit(df_option_prices):\n",
    "\n",
    "    df = df_option_prices.groupby(['Expiry_Bucket','BS_Delta_Bucket'],\n",
    "                                  observed=True).y.mean()\n",
    "\n",
    "    expiry_buckets  = df.index.get_level_values(0).unique().to_list() + ['Total']\n",
    "\n",
    "    df_result = pd.DataFrame(index=expiry_buckets,\n",
    "                             columns=['a', 'b', 'c'])\n",
    "\n",
    "    for expiry in expiry_buckets[:-1]:\n",
    "\n",
    "        c,b,a = np.polyfit(y=df.loc[expiry],\n",
    "                           x=df.loc[expiry].index,\n",
    "           deg=2)\n",
    "        \n",
    "        df_result.loc[expiry,['a', 'b', 'c']] = [a, b, c]\n",
    "    \n",
    "    c,b,a = np.polyfit(y=df_option_prices.groupby(['BS_Delta_Bucket'],\n",
    "                                                  observed=True).y.mean(),\n",
    "                        x=df_option_prices.groupby(['BS_Delta_Bucket'],\n",
    "                                                  observed=True).y.mean().index,\n",
    "        deg=2)\n",
    "    \n",
    "    df_result.loc['Total',['a', 'b', 'c']] = [a, b, c]\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set training and testing parameters\n",
    "\n",
    "TRAINING_PERIOD = 21*36 #36 months\n",
    "TESTING_PERIOD = 40 #2 months\n",
    "\n",
    "df_call_quote_dates = pd.Series(df_calls_diff.index.get_level_values(0).unique())\n",
    "\n",
    "import itertools\n",
    "\n",
    "lists = [\n",
    "   df_call_quote_dates.loc[TRAINING_PERIOD:].values,\n",
    "   df_calls_diff.index.get_level_values(2).unique().categories.to_list() + ['Total']\n",
    "   \n",
    "]\n",
    "\n",
    "df_call_params = pd.DataFrame(columns=['a','b','c'],\n",
    "                         index=pd.MultiIndex.from_tuples(itertools.product(*lists)))\n",
    "\n",
    "df_call_params.index.names = ['end_testing', 'expiry_bucket']\n",
    "\n",
    "df_call_params['start_testing'] = df_call_params.index.get_level_values(0).map(\n",
    "    {end:start for \n",
    "     end,start in \n",
    "     zip(df_call_quote_dates.loc[TRAINING_PERIOD:].values, \n",
    "         df_call_quote_dates.loc[:].values)})\n",
    "\n",
    "\n",
    "#Fitting a,b, and c on each training window\n",
    "for i in range(0, len(df_call_quote_dates)-TRAINING_PERIOD):\n",
    "\n",
    "    training_period = df_call_quote_dates.loc[i:i+TRAINING_PERIOD].values\n",
    "\n",
    "    df_call_params.loc[training_period[-1], ['a', 'b', 'c']] = quadratic_fit(df_calls_diff.loc[training_period]).values\n",
    "\n",
    "df_call_params['minus_b'] = -df_call_params['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=6, ncols=1, figsize=(25, 40))\n",
    "\n",
    "fig.suptitle('Rolling Estimation of Call Parameters - 36 months period', \n",
    "             fontsize=30)\n",
    "\n",
    "sns.lineplot(df_call_params.xs('1M',level='expiry_bucket')[['a','minus_b','c']]\n",
    "             ,ax=ax[0])\n",
    "ax[0].set_title('1 Month Expiry', fontsize=20)\n",
    "ax[0].set_xlabel(None)\n",
    "\n",
    "sns.lineplot(df_call_params.xs('3M',level='expiry_bucket')[['a','minus_b','c']]\n",
    "             ,ax=ax[1])\n",
    "ax[1].set_title('3 Months Expiry',fontsize=20)\n",
    "ax[1].set_xlabel(None)\n",
    "\n",
    "sns.lineplot(df_call_params.xs('6M',level='expiry_bucket')[['a','minus_b','c']]\n",
    "             ,ax=ax[2])\n",
    "ax[2].set_title('6 Months Expiry', fontsize=20)\n",
    "ax[2].set_xlabel(None)\n",
    "\n",
    "sns.lineplot(df_call_params.xs('9M',level='expiry_bucket')[['a','minus_b','c']]\n",
    "             ,ax=ax[3])\n",
    "ax[3].set_title('9 Months Expiry', fontsize=20)\n",
    "ax[3].set_xlabel(None)\n",
    "\n",
    "sns.lineplot(df_call_params.xs('12M',level='expiry_bucket')[['a','minus_b','c']]\n",
    "             ,ax=ax[4])\n",
    "ax[4].set_title('12 Months Expiry', fontsize=20)\n",
    "ax[4].set_xlabel(None)\n",
    "\n",
    "sns.lineplot(df_call_params.xs('Total',level='expiry_bucket')[['a','minus_b','c']]\n",
    "             ,ax=ax[5]\n",
    "             ,legend=True)\n",
    "\n",
    "ax[5].set_title('Total', fontsize=20)\n",
    "ax[5].set_xlabel('End of Testing Period', fontsize=11)\n",
    "\n",
    "ax_twin = ax[5].twinx()\n",
    "\n",
    "#ax_twin.set_yticks(np.linspace(ax_twin.get_yticks()[0], ax_twin.get_yticks()[-1], len(ax[5].get_yticks())))\n",
    "#ax.grid()\n",
    "\n",
    "sns.lineplot(df_calls_diff.groupby('QUOTE_DATE_t1').C_IV_t1.mean().rolling(TRAINING_PERIOD).mean(),\n",
    "             ax=ax_twin,\n",
    "             color=\"r\")\n",
    "\n",
    "\n",
    "\n",
    "ax_twin.legend(['Average Implied Volatility'], \n",
    "                loc=\"upper left\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see the parameters for maturities above 3M are very unstable due to its poor liquidity. <br> For expiries with more liquidity, the quadratic parameter (c) is negative on almost all cases, giving us an inverted parabola. <br>\n",
    "<br>\n",
    "On the overall trend, we can see a close relation between the average level of implied volatility in the period and the fitted factors.<br>In periods with lower (higher) volatility, the linear and quadratic factors tends to increase(decrease) in absolute values. <br>Thus, in higher volatility periods, the difference between MV hedging and BS hedging narrows. <br>This can also suggests a weaker relation, in periods of higher uncertainty, between expected changes in volatility and future returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puts_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y=df_puts_diff['P_DELTA_t1']*df_puts_diff['delta_S'],\n",
    "            x=df_puts_diff['delta_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set training and testing parameters\n",
    "\n",
    "TRAINING_PERIOD = 21*36 #36 months\n",
    "TESTING_PERIOD = 40 #2 months\n",
    "\n",
    "df_put_quote_dates = pd.Series(df_puts_diff.index.get_level_values(0).unique())\n",
    "\n",
    "import itertools\n",
    "\n",
    "lists = [\n",
    "   df_put_quote_dates.loc[TRAINING_PERIOD:].values,\n",
    "   df_puts_diff.index.get_level_values(2).unique().categories.to_list() + ['Total']\n",
    "   \n",
    "]\n",
    "\n",
    "df_put_params = pd.DataFrame(columns=['a','b','c'],\n",
    "                         index=pd.MultiIndex.from_tuples(itertools.product(*lists)))\n",
    "\n",
    "df_put_params.index.names = ['end_testing', 'expiry_bucket']\n",
    "\n",
    "df_put_params['start_testing'] = df_put_params.index.get_level_values(0).map(\n",
    "    {end:start for \n",
    "     end,start in \n",
    "     zip(df_put_quote_dates.loc[TRAINING_PERIOD:].values, \n",
    "         df_put_quote_dates.loc[:].values)})\n",
    "\n",
    "\n",
    "#Fitting a,b, and c on each training window\n",
    "for i in range(0, len(df_put_quote_dates)-TRAINING_PERIOD):\n",
    "\n",
    "    training_period = df_put_quote_dates.loc[i:i+TRAINING_PERIOD].values\n",
    "\n",
    "    df_put_params.loc[training_period[-1], ['a', 'b', 'c']] = quadratic_fit(df_puts_diff.loc[training_period], option_type='put').values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = 0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foo = df_puts_diff.loc[training_period]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_foo['delta_price']-df_foo['P_DELTA_t1']*df_foo['delta_S'])-(df_foo['P_VEGA_t1']*100)*(df_foo['delta_S_perc/sqrt_t'])*(a+b*df_foo['P_DELTA_t1']+c*(df_foo['P_DELTA_t1']**2)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_foo['delta_price']/df_foo['delta_S']-df_foo['P_DELTA_t1']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df_foo.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_helio(params,df):\n",
    "\n",
    "    a = params[0]\n",
    "    b = params[1]\n",
    "    c = params[2]\n",
    "\n",
    "    \n",
    "    diff = (df['delta_price']-df['P_DELTA_t1']*df['delta_S'])-(df['P_VEGA_t1']*100)*(df['delta_S_perc/sqrt_t'])*(a+b*(df['P_DELTA_t1'])+c*(df_foo['P_DELTA_t1']**2))\n",
    "    return sum(diff**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puts_diff.loc[training_period]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puts_diff.loc[training_period].xs('3M',level='Expiry_Bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puts_diff.groupby('Expiry_Bucket').UNDERLYING_LAST_t1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.optimize.minimize(see_helio,\n",
    "                        x0=np.array([0.0, 0.0, 0.0]),\n",
    "                        method='SLSQP',\n",
    "                        args=df_puts_diff.loc[training_period].xs('6M',level='Expiry_Bucket'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=6, ncols=1, figsize=(25, 40))\n",
    "\n",
    "fig.suptitle('Rolling Estimation of Put Parameters - 36 months period', \n",
    "             fontsize=30)\n",
    "\n",
    "sns.lineplot(df_put_params.xs('1M',level='expiry_bucket')[['a','b','c']]\n",
    "             ,ax=ax[0])\n",
    "ax[0].set_title('1 Month Expiry', fontsize=20)\n",
    "ax[0].set_xlabel(None)\n",
    "\n",
    "sns.lineplot(df_put_params.xs('3M',level='expiry_bucket')[['a','b','c']]\n",
    "             ,ax=ax[1])\n",
    "ax[1].set_title('3 Months Expiry',fontsize=20)\n",
    "ax[1].set_xlabel(None)\n",
    "\n",
    "sns.lineplot(df_put_params.xs('6M',level='expiry_bucket')[['a','b','c']]\n",
    "             ,ax=ax[2])\n",
    "ax[2].set_title('6 Months Expiry', fontsize=20)\n",
    "ax[2].set_xlabel(None)\n",
    "\n",
    "sns.lineplot(df_put_params.xs('9M',level='expiry_bucket')[['a','b','c']]\n",
    "             ,ax=ax[3])\n",
    "ax[3].set_title('9 Months Expiry', fontsize=20)\n",
    "ax[3].set_xlabel(None)\n",
    "\n",
    "sns.lineplot(df_put_params.xs('12M',level='expiry_bucket')[['a','b','c']]\n",
    "             ,ax=ax[4])\n",
    "ax[4].set_title('12 Months Expiry', fontsize=20)\n",
    "ax[4].set_xlabel(None)\n",
    "\n",
    "sns.lineplot(df_put_params.xs('Total',level='expiry_bucket')[['a','b','c']]\n",
    "             ,ax=ax[5]\n",
    "             ,legend=True)\n",
    "\n",
    "ax[5].set_title('Total', fontsize=20)\n",
    "ax[5].set_xlabel('End of Testing Period', fontsize=11)\n",
    "\n",
    "ax_twin = ax[5].twinx()\n",
    "\n",
    "#ax_twin.set_yticks(np.linspace(ax_twin.get_yticks()[0], ax_twin.get_yticks()[-1], len(ax[5].get_yticks())))\n",
    "#ax.grid()\n",
    "\n",
    "sns.lineplot(df_puts_diff.groupby('QUOTE_DATE_t1').P_IV_t1.mean().rolling(TRAINING_PERIOD).mean(),\n",
    "             ax=ax_twin,\n",
    "             color=\"r\")\n",
    "\n",
    "ax_twin.legend(['Average Implied Volatility'], \n",
    "                loc=\"upper left\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_IV = df_calls_diff.copy()\n",
    "\n",
    "df_plot_IV = df_plot_IV.delta_IV*np.sqrt(df_plot_IV.DTE_t1/360)/df_plot_IV['delta_S_perc']\n",
    "df_plot_IV = df_plot_IV[(df_plot_IV > cut_min) & (df_plot_IV < cut_max)].copy()\n",
    "\n",
    "df_plot_IV.name = 'adjusted_exp_IV'\n",
    "df_plot_IV = df_plot_IV.to_frame().reset_index()\n",
    "\n",
    "df_plot_IV['BS_Delta_Bucket'] = df_plot_IV['BS_Delta_Bucket'].astype('category')\n",
    "df_plot_IV_line = df_plot_IV.groupby('BS_Delta_Bucket',observed=False).adjusted_exp_IV.median().to_frame().reset_index()\n",
    "\n",
    "c,b,a = np.polyfit(y=df_plot_IV_line.adjusted_exp_IV,\n",
    "                    x=df_plot_IV_line.BS_Delta_Bucket,\n",
    "    deg=2)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 8))\n",
    "\n",
    "ax.set_title('Adjusted Implied Volatility change vs Option Delta',\n",
    "             fontsize=17)\n",
    "\n",
    "sns.boxplot(x='BS_Delta_Bucket',\n",
    "            y='adjusted_exp_IV',\n",
    "            #hue='Expiry_Bucket',\n",
    "            data=df_plot_IV,\n",
    "            showfliers = False,\n",
    "            palette=\"husl\",\n",
    "            hue='BS_Delta_Bucket',\n",
    "            legend=False,\n",
    "            saturation=1)\n",
    "\n",
    "sns.pointplot(x='BS_Delta_Bucket',\n",
    "              y='adjusted_exp_IV',\n",
    "              data=df_plot_IV_line,\n",
    "              linestyles='--', \n",
    "              scale=0.4, \n",
    "              color='k', \n",
    "              err_kws={'linewidth': 0}, \n",
    "              capsize=0)\n",
    "\n",
    "ax.text(x = 1, \n",
    "        y = df_plot_IV.adjusted_exp_IV.min(), \n",
    "        s = r'Fitted Parabola (dashed):'+'\\n'+ r' $'+str(np.round(c,2))+r'\\delta^2_{bs}+' + str(np.round(b,2))+r'\\delta_{bs} ' + str(np.round(a,2))+r'$', \n",
    "        horizontalalignment='center', \n",
    "        #verticalalignment='top', \n",
    "        bbox=props,\n",
    "        fontsize=17)\n",
    "\n",
    "plt.ylabel(r'$\\mathbb{E}[\\Delta \\sigma_{imp}]\\ \\sqrt{T}\\ \\frac{S}{\\Delta S}$', fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_IV = df_puts_diff.copy()\n",
    "\n",
    "df_plot_IV = df_plot_IV.delta_IV*np.sqrt(df_plot_IV.DTE_t1/360)/df_plot_IV['delta_S_perc']\n",
    "df_plot_IV = df_plot_IV[(df_plot_IV > cut_min) & (df_plot_IV < cut_max)].copy()\n",
    "\n",
    "df_plot_IV.name = 'adjusted_exp_IV'\n",
    "df_plot_IV = df_plot_IV.to_frame().reset_index()\n",
    "\n",
    "df_plot_IV['BS_Delta_Bucket'] = df_plot_IV['BS_Delta_Bucket'].astype('category')\n",
    "df_plot_IV_line = df_plot_IV.groupby('BS_Delta_Bucket',observed=False).adjusted_exp_IV.median().to_frame().reset_index()\n",
    "\n",
    "c,b,a = np.polyfit(y=df_plot_IV_line.adjusted_exp_IV,\n",
    "                    x=df_plot_IV_line.BS_Delta_Bucket,\n",
    "    deg=2)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 8))\n",
    "\n",
    "ax.set_title('Adjusted Implied Volatility change vs Option Delta',\n",
    "             fontsize=17)\n",
    "\n",
    "sns.boxplot(x='BS_Delta_Bucket',\n",
    "            y='adjusted_exp_IV',\n",
    "            #hue='Expiry_Bucket',\n",
    "            data=df_plot_IV,\n",
    "            showfliers = False,\n",
    "            palette=\"husl\",\n",
    "            hue='BS_Delta_Bucket',\n",
    "            legend=False,\n",
    "            saturation=1)\n",
    "\n",
    "sns.pointplot(x='BS_Delta_Bucket',\n",
    "              y='adjusted_exp_IV',\n",
    "              data=df_plot_IV_line,\n",
    "              linestyles='--', \n",
    "              scale=0.4, \n",
    "              color='k', \n",
    "              err_kws={'linewidth': 0}, \n",
    "              capsize=0)\n",
    "\n",
    "ax.text(x = 1, \n",
    "        y = df_plot_IV.adjusted_exp_IV.min(), \n",
    "        s = r'Fitted Parabola (dashed):'+'\\n'+ r' $'+str(np.round(c,2))+r'\\delta^2_{bs}+' + str(np.round(b,2))+r'\\delta_{bs} ' + str(np.round(a,2))+r'$', \n",
    "        horizontalalignment='center', \n",
    "        #verticalalignment='top', \n",
    "        bbox=props,\n",
    "        fontsize=17)\n",
    "\n",
    "plt.ylabel(r'$\\mathbb{E}[\\Delta \\sigma_{imp}]\\ \\sqrt{T}\\ \\frac{S}{\\Delta S}$', fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above shows the relation of adjusted changes in volatility and BS delta for every trading day on the dataset. <br> We can see a parabolic trend, but most of the varition cannot be explained by this simples relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. $\\mathbb{E}[\\Delta \\sigma_{imp}]$ expected to be negative but might not be. <br> \n",
    "Is your achieved hedging Gain anywhere close to 15% and in which delta buckets and expiries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING_PERIOD = 40 #2 months\n",
    "\n",
    "#declare df_gain_n_observations\n",
    "lists = [\n",
    "   df_call_quote_dates.loc[TRAINING_PERIOD+1:].iloc[:-TESTING_PERIOD],\n",
    "   sorted(df_calls_diff.index.get_level_values(1).unique())\n",
    "   ]\n",
    "\n",
    "df_gain_n_observations = pd.DataFrame(index=pd.MultiIndex.from_tuples(itertools.product(*lists)),\n",
    "                                      columns=df_calls_diff.index.get_level_values(2).unique().categories)\n",
    "\n",
    "df_gain_n_observations.index.names = ['start_testing', 'delta_bucket']\n",
    "\n",
    "#declare df_r_squared\n",
    "lists = [\n",
    "   df_call_quote_dates.loc[TRAINING_PERIOD+1:].iloc[:-TESTING_PERIOD],\n",
    "   sorted(df_calls_diff.index.get_level_values(1).unique()) + ['Total']\n",
    "   ]\n",
    "\n",
    "df_r_squared = pd.DataFrame(index=df_call_quote_dates.loc[TRAINING_PERIOD+1:].iloc[:-TESTING_PERIOD],\n",
    "                            columns=sorted(df_calls_diff.index.get_level_values(1).unique()))\n",
    "\n",
    "lists = [\n",
    "   df_call_quote_dates.loc[TRAINING_PERIOD+1:].iloc[:-TESTING_PERIOD],\n",
    "   sorted(df_calls_diff.index.get_level_values(1).unique()) + ['Total']\n",
    "   ]\n",
    "\n",
    "#df_gain_mv\n",
    "df_gain_mv = pd.DataFrame(index=pd.MultiIndex.from_tuples(itertools.product(*lists)),\n",
    "                          columns=df_calls_diff.index.get_level_values(2).unique().categories)\n",
    "\n",
    "df_gain_mv.index.names = ['start_testing','delta_bucket']\n",
    "df_gain_mv['Total'] = np.nan\n",
    "\n",
    "for i in range(0, len(df_call_quote_dates)-TRAINING_PERIOD-TESTING_PERIOD-1):\n",
    "    \n",
    "        testing_period = df_call_quote_dates.loc[i+TRAINING_PERIOD: i+TRAINING_PERIOD+TESTING_PERIOD].values\n",
    "\n",
    "        df_loop = pd.merge(df_calls_diff.loc[testing_period[1:]].reset_index(),\n",
    "                           df_call_params.loc[testing_period[0],['a','b','c']].reset_index(),\n",
    "                           left_on = 'Expiry_Bucket',\n",
    "                           right_on = 'expiry_bucket'\n",
    "                           )\n",
    "\n",
    "        df_loop['epsilon_bs'] = df_loop['delta_price'] - df_loop['BS_Delta_Bucket']*df_loop['delta_S']\n",
    "\n",
    "        df_loop['Delta_MV'] = df_loop['BS_Delta_Bucket'] + (df_loop['C_VEGA_t1']*100\n",
    "                                                            *(df_loop['a']+df_loop['b']*df_loop['BS_Delta_Bucket']+df_loop['c']*(df_loop['BS_Delta_Bucket']**2))\n",
    "                                                            /(np.sqrt(df_loop['DTE_t1']/360)*df_loop['UNDERLYING_LAST_t1']))\n",
    "\n",
    "        df_loop['epsilon_mv'] = df_loop['delta_price'] - df_loop['Delta_MV']*df_loop['delta_S']\n",
    "\n",
    "        df_epsilon_bs = (df_loop.groupby(['BS_Delta_Bucket','Expiry_Bucket'],\n",
    "                                        observed=True)\n",
    "                                        .apply(lambda x: sum((x.epsilon_bs)**2))\n",
    "                                        .unstack())\n",
    "\n",
    "        df_epsilon_bs['Total'] = (df_loop\n",
    "                                        .groupby('BS_Delta_Bucket',\n",
    "                                                observed=True)\n",
    "                                                .apply(lambda x: sum((x.epsilon_bs)**2)))\n",
    "\n",
    "\n",
    "        df_epsilon_bs.loc['Total'] = (df_loop\n",
    "                                      .groupby('Expiry_Bucket',\n",
    "                                               observed=True)\n",
    "                                               .apply(lambda x: sum((x.epsilon_bs)**2)))\n",
    "\n",
    "        df_epsilon_bs.loc['Total', 'Total'] = sum(df_loop['epsilon_bs']**2)\n",
    "\n",
    "        df_epsilon_mv = (df_loop.groupby(['BS_Delta_Bucket','Expiry_Bucket'],\n",
    "                                        observed=True)\n",
    "                                        .apply(lambda x: sum((x.epsilon_mv)**2))\n",
    "                                        .unstack())\n",
    "\n",
    "        df_epsilon_mv['Total'] = (df_loop\n",
    "                                        .groupby('BS_Delta_Bucket',\n",
    "                                                observed=True)\n",
    "                                                .apply(lambda x: sum((x.epsilon_mv)**2)))\n",
    "\n",
    "\n",
    "        df_epsilon_mv.loc['Total'] = (df_loop\n",
    "                                        .groupby('Expiry_Bucket',\n",
    "                                                observed=True)\n",
    "                                                .apply(lambda x: sum((x.epsilon_mv)**2)))\n",
    "\n",
    "        df_epsilon_mv.loc['Total', 'Total'] = sum(df_loop['epsilon_mv']**2)\n",
    "\n",
    "        if len(df_epsilon_mv.columns) < len(df_gain_mv.loc[testing_period[1]].columns):\n",
    "\n",
    "                for col in set(df_gain_mv.loc[testing_period[1]].columns)-set(df_epsilon_mv.columns):\n",
    "\n",
    "                        df_epsilon_mv[col] = np.nan\n",
    "\n",
    "        if len(df_epsilon_bs.columns) < len(df_gain_mv.loc[testing_period[1]].columns):\n",
    "\n",
    "                for col in set(df_gain_mv.loc[testing_period[1]].columns)-set(df_epsilon_bs.columns):\n",
    "\n",
    "                        df_epsilon_bs[col] = np.nan\n",
    "\n",
    "        df_epsilon_bs = df_epsilon_bs[df_gain_mv.loc[testing_period[1]].columns]\n",
    "        df_epsilon_mv = df_epsilon_mv[df_gain_mv.loc[testing_period[1]].columns]\n",
    "\n",
    "        df_gain_mv.loc[testing_period[1]].loc[df_epsilon_mv.index] = (1 - df_epsilon_mv/df_epsilon_bs)\n",
    "        \n",
    "        df_loop_observations = (df_loop.groupby(['BS_Delta_Bucket','Expiry_Bucket'], observed=True)\n",
    "                                .QUOTE_DATE_t1.count()\n",
    "                                .unstack())\n",
    "        \n",
    "        if len(df_loop_observations.columns) < len(df_gain_n_observations.loc[testing_period[1]].columns):\n",
    "\n",
    "                for col in set(df_gain_n_observations.loc[testing_period[1]].columns)-set(df_loop_observations.columns):\n",
    "\n",
    "                        df_loop_observations[col] = np.nan\n",
    "\n",
    "        df_loop_observations = df_loop_observations[df_gain_n_observations.loc[testing_period[1]].columns]\n",
    "        \n",
    "        df_gain_n_observations.loc[testing_period[1]].loc[sorted(df_loop.BS_Delta_Bucket.unique())] = df_loop_observations\n",
    "        df_r_squared.loc[testing_period[1]] = get_r_squared(df_loop.set_index(['QUOTE_DATE_t1','BS_Delta_Bucket', 'Expiry_Bucket']))\n",
    "\n",
    "\n",
    "df_gain_mv = df_gain_mv.astype(float)\n",
    "df_r_squared = df_r_squared.astype(float)\n",
    "df_gain_n_observations = df_gain_n_observations.astype(float)\n",
    "\n",
    "df_gain_n_observations['Total'] = df_gain_n_observations.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_raw_string(number, format):\n",
    "    \n",
    "    if format == 'percentage':\n",
    "        return f'{number:.1f}' + r'\\%'\n",
    "    if format == 'float_2digits':\n",
    "        return f'{number:.2f}'\n",
    "    if format == 'float_1digit':\n",
    "        return f'{number:.1f}'\n",
    "    if format == 'int':\n",
    "        return f'{int(number):,}'\n",
    "\n",
    "\n",
    "def dataframe_to_latex(df,title,format,total):\n",
    "\n",
    "    n_rows, n_cols = df.shape\n",
    "\n",
    "    str_latex = r\"\"\"$$ \n",
    "    \\begin{aligned}\n",
    "    & \\text {\"\"\" + title + r\"\"\"}\\\\\n",
    "    &\\begin{array}{|c|r|r|r|r|r|r|}\n",
    "    \\hline \\hline \n",
    "    \\text { \"\"\"+df.index.name.replace('_',' ') +r\"\"\"} \"\"\"+''.join([\"& \\t {\"+x+\"} \" for x in df.columns])+r\"\"\"\n",
    "    \n",
    "    \\\\\n",
    "    \\hline \"\"\"\n",
    "\n",
    "    for i in df.index:\n",
    "\n",
    "        if total:\n",
    "            if i !=df.index[-1]:\n",
    "                str_latex=str_latex+str(i)+r\"\"\"& \"\"\" \n",
    "            else:\n",
    "                \n",
    "                str_latex=str_latex+r\"\"\"\\hline \"\"\"+str(df.loc[i].name)+r\"\"\" &\"\"\"\n",
    "\n",
    "        else:\n",
    "            str_latex=str_latex+str(i)+r\"\"\"& \"\"\" \n",
    "\n",
    "        for j in df.columns:\n",
    "            if j  != df.columns[-1]: \n",
    "                str_latex=str_latex + \\\n",
    "                    float_to_raw_string(df.loc[i,j],format) + r'&'\n",
    "            else:\n",
    "                str_latex=str_latex + float_to_raw_string(df.loc[i,j],format)\n",
    "        str_latex=str_latex+r\"\"\"\\\\ \"\"\"\n",
    "\n",
    "\n",
    "    str_latex=str_latex+r\"\"\"\\hline\n",
    "\n",
    "    \\end{array}\n",
    "    \\end{aligned}\n",
    "    $$\"\"\"\n",
    "\n",
    "    display(Latex(str_latex))\n",
    "\n",
    "# bool_array = np.diff([x.year*10+x.month for x in df_gain_mv.index.get_level_values(0).unique()])\n",
    "# bool_array = [bool(x) for x in bool_array]\n",
    "\n",
    "# first_days = pd.Series(df_gain_mv.index.get_level_values(0).unique())[1:][bool_array].values\n",
    "# df_gain_mv_summary = df_gain_mv.loc[first_days].groupby('delta_bucket').mean()*100\n",
    "df_gain_mv_summary = df_gain_mv.groupby('delta_bucket').mean()*100\n",
    "\n",
    "df_n_observations = df_gain_n_observations.fillna(0).groupby('delta_bucket').mean()\n",
    "df_n_observations.loc['Total'] = df_n_observations.sum()\n",
    "\n",
    "dataframe_to_latex(df=df_gain_mv_summary,\n",
    "                   title='Average gains for each delta and expiry buckets',\n",
    "                   format='percentage',\n",
    "                   total=True)\n",
    "\n",
    "dataframe_to_latex(df_n_observations,\n",
    "                   title='Average daily comparisons for each bucket',\n",
    "                   format='int',\n",
    "                   total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loop.groupby(['BS_Delta_Bucket','Expiry_Bucket'])[['Delta_MV', 'BS_Delta_Bucket_t0']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loop['P_VEGA_t1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loop.iloc[0].UNDERLYING_LAST_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_puts_diff['P_BS_t0']-df_puts_diff['P_LAST_t0']).groupby('QUOTE_DATE_t1').mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_loop['P_VEGA_t1']*100\n",
    "                                                    *(df_loop['a']+df_loop['b']*df_loop['BS_Delta_Bucket']+df_loop['c']*(df_loop['BS_Delta_Bucket']**2))\n",
    "                                                    /(np.sqrt(df_loop['DTE_t1']/360)*df_loop['UNDERLYING_LAST_t1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epsilon_mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epsilon_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gain_mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_loop[['epsilon_bs','epsilon_mv']])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_squared.mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_loop['P_VEGA_t1']*100\n",
    "                                                            *(df_loop['a']+df_loop['b']*df_loop['BS_Delta_Bucket']+df_loop['c']*(df_loop['BS_Delta_Bucket']**2))\n",
    "                                                            /(np.sqrt(df_loop['DTE_t1']/360)*df_loop['UNDERLYING_LAST_t1'])).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING_PERIOD = 40 #2 months\n",
    "\n",
    "#declare df_gain_n_observations\n",
    "lists = [\n",
    "   df_put_quote_dates.loc[TRAINING_PERIOD+1:].iloc[:-TESTING_PERIOD],\n",
    "   sorted(df_puts_diff.index.get_level_values(1).unique())\n",
    "   ]\n",
    "\n",
    "df_gain_n_observations = pd.DataFrame(index=pd.MultiIndex.from_tuples(itertools.product(*lists)),\n",
    "                                      columns=df_puts_diff.index.get_level_values(2).unique().categories)\n",
    "\n",
    "df_gain_n_observations.index.names = ['start_testing', 'delta_bucket']\n",
    "\n",
    "#declare df_r_squared\n",
    "lists = [\n",
    "   df_put_quote_dates.loc[TRAINING_PERIOD+1:].iloc[:-TESTING_PERIOD],\n",
    "   sorted(df_puts_diff.index.get_level_values(1).unique()) + ['Total']\n",
    "   ]\n",
    "\n",
    "df_r_squared = pd.DataFrame(index=df_put_quote_dates.loc[TRAINING_PERIOD+1:].iloc[:-TESTING_PERIOD],\n",
    "                            columns=sorted(df_puts_diff.index.get_level_values(1).unique()))\n",
    "\n",
    "lists = [\n",
    "   df_put_quote_dates.loc[TRAINING_PERIOD+1:].iloc[:-TESTING_PERIOD],\n",
    "   sorted(df_puts_diff.index.get_level_values(1).unique()) + ['Total']\n",
    "   ]\n",
    "\n",
    "#df_gain_mv\n",
    "df_gain_mv = pd.DataFrame(index=pd.MultiIndex.from_tuples(itertools.product(*lists)),\n",
    "                          columns=df_puts_diff.index.get_level_values(2).unique().categories)\n",
    "\n",
    "df_gain_mv.index.names = ['start_testing','delta_bucket']\n",
    "df_gain_mv['Total'] = np.nan\n",
    "\n",
    "for i in range(0, len(df_put_quote_dates)-TRAINING_PERIOD-TESTING_PERIOD-1):\n",
    "    \n",
    "        testing_period = df_put_quote_dates.loc[i+TRAINING_PERIOD: i+TRAINING_PERIOD+TESTING_PERIOD].values\n",
    "\n",
    "        df_loop = pd.merge(df_puts_diff.loc[testing_period[1:]].reset_index(),\n",
    "                           df_put_params.loc[testing_period[0],['a','b','c']].reset_index(),\n",
    "                           left_on = 'Expiry_Bucket',\n",
    "                           right_on = 'expiry_bucket'\n",
    "                           )\n",
    "\n",
    "        df_loop['epsilon_bs'] = df_loop['delta_price'] - df_loop['BS_Delta_Bucket']*df_loop['delta_S']\n",
    "\n",
    "        df_loop['Delta_MV'] = df_loop['BS_Delta_Bucket'] + (df_loop['P_VEGA_t1']*100\n",
    "                                                            *(df_loop['a']+df_loop['b']*(1+df_loop['BS_Delta_Bucket'])+df_loop['c']*((1+df_loop['BS_Delta_Bucket'])**2))\n",
    "                                                            /(np.sqrt(df_loop['DTE_t1']/360)*df_loop['UNDERLYING_LAST_t1']))\n",
    "\n",
    "        df_loop['epsilon_mv'] = df_loop['delta_price'] - df_loop['Delta_MV']*df_loop['delta_S']\n",
    "\n",
    "        df_epsilon_bs = (df_loop.groupby(['BS_Delta_Bucket','Expiry_Bucket'],\n",
    "                                        observed=True)\n",
    "                                        .apply(lambda x: sum((x.epsilon_bs)**2))\n",
    "                                        .unstack())\n",
    "\n",
    "        df_epsilon_bs['Total'] = (df_loop\n",
    "                                  .groupby('BS_Delta_Bucket',\n",
    "                                           observed=True)\n",
    "                                           .apply(lambda x: sum((x.epsilon_bs)**2)))\n",
    "\n",
    "\n",
    "        df_epsilon_bs.loc['Total'] = (df_loop\n",
    "                                      .groupby('Expiry_Bucket',\n",
    "                                               observed=True)\n",
    "                                               .apply(lambda x: sum((x.epsilon_bs)**2)))\n",
    "\n",
    "        df_epsilon_bs.loc['Total', 'Total'] = sum(df_loop['epsilon_bs']**2)\n",
    "\n",
    "        df_epsilon_mv = (df_loop.groupby(['BS_Delta_Bucket','Expiry_Bucket'],\n",
    "                                        observed=True)\n",
    "                                        .apply(lambda x: sum((x.epsilon_mv)**2))\n",
    "                                        .unstack())\n",
    "\n",
    "        df_epsilon_mv['Total'] = (df_loop\n",
    "                                        .groupby('BS_Delta_Bucket',\n",
    "                                                observed=True)\n",
    "                                                .apply(lambda x: sum((x.epsilon_mv)**2)))\n",
    "\n",
    "\n",
    "        df_epsilon_mv.loc['Total'] = (df_loop\n",
    "                                        .groupby('Expiry_Bucket',\n",
    "                                                observed=True)\n",
    "                                                .apply(lambda x: sum((x.epsilon_mv)**2)))\n",
    "\n",
    "        df_epsilon_mv.loc['Total', 'Total'] = sum(df_loop['epsilon_mv']**2)\n",
    "\n",
    "        if len(df_epsilon_mv.columns) < len(df_gain_mv.loc[testing_period[1]].columns):\n",
    "\n",
    "                for col in set(df_gain_mv.loc[testing_period[1]].columns)-set(df_epsilon_mv.columns):\n",
    "\n",
    "                        df_epsilon_mv[col] = np.nan\n",
    "\n",
    "        if len(df_epsilon_bs.columns) < len(df_gain_mv.loc[testing_period[1]].columns):\n",
    "\n",
    "                for col in set(df_gain_mv.loc[testing_period[1]].columns)-set(df_epsilon_bs.columns):\n",
    "\n",
    "                        df_epsilon_bs[col] = np.nan\n",
    "\n",
    "        df_epsilon_bs = df_epsilon_bs[df_gain_mv.loc[testing_period[1]].columns]\n",
    "        df_epsilon_mv = df_epsilon_mv[df_gain_mv.loc[testing_period[1]].columns]\n",
    "\n",
    "        df_gain_mv.loc[testing_period[1]].loc[df_epsilon_mv.index] = (1 - df_epsilon_mv/df_epsilon_bs)\n",
    "        \n",
    "        df_loop_observations = (df_loop.groupby(['BS_Delta_Bucket','Expiry_Bucket'], observed=True)\n",
    "                                .QUOTE_DATE_t1.count()\n",
    "                                .unstack())\n",
    "        \n",
    "        if len(df_loop_observations.columns) < len(df_gain_n_observations.loc[testing_period[1]].columns):\n",
    "\n",
    "                for col in set(df_gain_n_observations.loc[testing_period[1]].columns)-set(df_loop_observations.columns):\n",
    "\n",
    "                        df_loop_observations[col] = np.nan\n",
    "\n",
    "        df_loop_observations = df_loop_observations[df_gain_n_observations.loc[testing_period[1]].columns]\n",
    "        \n",
    "        df_gain_n_observations.loc[testing_period[1]].loc[sorted(df_loop.BS_Delta_Bucket.unique())] = df_loop_observations\n",
    "        df_r_squared.loc[testing_period[1]] = get_r_squared(df_loop.set_index(['QUOTE_DATE_t1','BS_Delta_Bucket', 'Expiry_Bucket']))\n",
    "        \n",
    "        # #remove data with less than 10 comparisons in the period\n",
    "        # df_n_observations = df_loop.groupby(['BS_Delta_Bucket','Expiry_Bucket'], observed=True).QUOTE_DATE_t1.count().unstack()\n",
    "        # df_n_observations['Total'] = df_n_observations.sum(axis=1)\n",
    "\n",
    "        # df_n_observations = df_n_observations[df_gain_mv.loc[testing_period[1]].columns]\n",
    "        # df_gain_mv.loc[testing_period[0]][df_n_observations < 10] = np.nan\n",
    "\n",
    "df_gain_mv = df_gain_mv.astype(float)\n",
    "df_r_squared = df_r_squared.astype(float)\n",
    "df_gain_n_observations = df_gain_n_observations.astype(float)\n",
    "\n",
    "df_gain_n_observations['Total'] = df_gain_n_observations.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gain_mv.xs('Total', level='delta_bucket')['Total'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gain_mv_summary = df_gain_mv.groupby('delta_bucket').mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gain_mv_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_raw_string(number, format):\n",
    "    \n",
    "    if format == 'percentage':\n",
    "        return f'{number:.1f}' + r'\\%'\n",
    "    if format == 'float_2digits':\n",
    "        return f'{number:.2f}'\n",
    "    if format == 'float_1digit':\n",
    "        return f'{number:.1f}'\n",
    "    if format == 'int':\n",
    "        return f'{int(number):,}'\n",
    "\n",
    "\n",
    "def dataframe_to_latex(df,title,format,total):\n",
    "\n",
    "    n_rows, n_cols = df.shape\n",
    "\n",
    "    str_latex = r\"\"\"$$ \n",
    "    \\begin{aligned}\n",
    "    & \\text {\"\"\" + title + r\"\"\"}\\\\\n",
    "    &\\begin{array}{|c|r|r|r|r|r|r|}\n",
    "    \\hline \\hline \n",
    "    \\text { \"\"\"+df.index.name.replace('_',' ') +r\"\"\"} \"\"\"+''.join([\"& \\t {\"+x+\"} \" for x in df.columns])+r\"\"\"\n",
    "    \n",
    "    \\\\\n",
    "    \\hline \"\"\"\n",
    "\n",
    "    for i in df.index:\n",
    "\n",
    "        if total:\n",
    "            if i !=df.index[-1]:\n",
    "                str_latex=str_latex+str(i)+r\"\"\"& \"\"\" \n",
    "            else:\n",
    "                \n",
    "                str_latex=str_latex+r\"\"\"\\hline \"\"\"+str(df.loc[i].name)+r\"\"\" &\"\"\"\n",
    "\n",
    "        else:\n",
    "            str_latex=str_latex+str(i)+r\"\"\"& \"\"\" \n",
    "\n",
    "        for j in df.columns:\n",
    "            if j  != df.columns[-1]: \n",
    "                str_latex=str_latex + \\\n",
    "                    float_to_raw_string(df.loc[i,j],format) + r'&'\n",
    "            else:\n",
    "                str_latex=str_latex + float_to_raw_string(df.loc[i,j],format)\n",
    "        str_latex=str_latex+r\"\"\"\\\\ \"\"\"\n",
    "\n",
    "\n",
    "    str_latex=str_latex+r\"\"\"\\hline\n",
    "\n",
    "    \\end{array}\n",
    "    \\end{aligned}\n",
    "    $$\"\"\"\n",
    "\n",
    "    display(Latex(str_latex))\n",
    "\n",
    "# bool_array = np.diff([x.year*10+x.month for x in df_gain_mv.index.get_level_values(0).unique()])\n",
    "# bool_array = [bool(x) for x in bool_array]\n",
    "\n",
    "# first_days = pd.Series(df_gain_mv.index.get_level_values(0).unique())[1:][bool_array].values\n",
    "# df_gain_mv_summary = df_gain_mv.loc[first_days].groupby('delta_bucket').mean()*100\n",
    "df_gain_mv_summary = df_gain_mv.groupby('delta_bucket').mean()*100\n",
    "\n",
    "df_n_observations = df_gain_n_observations.fillna(0).groupby('delta_bucket').mean()\n",
    "df_n_observations.loc['Total'] = df_n_observations.sum()\n",
    "\n",
    "dataframe_to_latex(df=df_gain_mv_summary,\n",
    "                   title='Average gains for each delta and expiry buckets',\n",
    "                   format='percentage',\n",
    "                   total=True)\n",
    "\n",
    "dataframe_to_latex(df_n_observations,\n",
    "                   title='Average daily comparisons for each bucket',\n",
    "                   format='int',\n",
    "                   total=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from the Hull & White paper, we get better gains for ITM calls and specially poor values for deep OTM calls (.1 delta) <br>\n",
    "The overall gain is satisfactory (above 15%) <br>\n",
    "\n",
    "The liquidity on options with expiry buckets greater than 6M is lower, specially for deep OTM and deep ITM options, this can cause negative gains on these buckets, but their errors do not have a great impact on the total gain. <br>\n",
    "\n",
    "More robust results can be found in ATM options (delta between .4 and .6) on the 1M and 3M buckets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
